{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a5f930f",
   "metadata": {},
   "source": [
    "<h3>\n",
    "Check System Resources and Capabilities\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c77cb9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SYSTEM INFORMATION ===\n",
      "System: Darwin\n",
      "Machine: x86_64\n",
      "Processor: i386\n",
      "Python Version: 3.11.7\n",
      "\n",
      "=== MEMORY INFORMATION ===\n",
      "Total Memory: 16.00 GB\n",
      "Available Memory: 1.62 GB\n",
      "Memory Usage: 89.9%\n",
      "\n",
      "=== CPU INFORMATION ===\n",
      "Physical CPU Cores: 10\n",
      "Logical CPU Cores: 10\n",
      "Current CPU Usage: 16.8%\n",
      "\n",
      "=== DISK INFORMATION ===\n",
      "Total Disk Space: 460.43 GB\n",
      "Free Disk Space: 107.47 GB\n",
      "Disk Usage: 2.3%\n",
      "\n",
      "=== RECOMMENDED SETTINGS FOR YOUR SYSTEM ===\n",
      "Recommended Dask Workers: 9\n",
      "Recommended Memory per Worker: 2 GB\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "import platform\n",
    "import os\n",
    "\n",
    "# Check system information\n",
    "print(\"=== SYSTEM INFORMATION ===\")\n",
    "print(f\"System: {platform.system()}\")\n",
    "print(f\"Machine: {platform.machine()}\")\n",
    "print(f\"Processor: {platform.processor()}\")\n",
    "print(f\"Python Version: {platform.python_version()}\")\n",
    "\n",
    "# Check memory\n",
    "memory = psutil.virtual_memory()\n",
    "print(f\"\\n=== MEMORY INFORMATION ===\")\n",
    "print(f\"Total Memory: {memory.total / (1024**3):.2f} GB\")\n",
    "print(f\"Available Memory: {memory.available / (1024**3):.2f} GB\")\n",
    "print(f\"Memory Usage: {memory.percent:.1f}%\")\n",
    "\n",
    "# Check CPU\n",
    "print(f\"\\n=== CPU INFORMATION ===\")\n",
    "print(f\"Physical CPU Cores: {psutil.cpu_count(logical=False)}\")\n",
    "print(f\"Logical CPU Cores: {psutil.cpu_count(logical=True)}\")\n",
    "print(f\"Current CPU Usage: {psutil.cpu_percent(interval=1)}%\")\n",
    "\n",
    "# Check disk space\n",
    "disk = psutil.disk_usage('/')\n",
    "print(f\"\\n=== DISK INFORMATION ===\")\n",
    "print(f\"Total Disk Space: {disk.total / (1024**3):.2f} GB\")\n",
    "print(f\"Free Disk Space: {disk.free / (1024**3):.2f} GB\")\n",
    "print(f\"Disk Usage: {(disk.used / disk.total) * 100:.1f}%\")\n",
    "\n",
    "print(f\"\\n=== RECOMMENDED SETTINGS FOR YOUR SYSTEM ===\")\n",
    "recommended_workers = max(1, psutil.cpu_count(logical=False) - 1)\n",
    "recommended_memory_per_worker = max(2, int((memory.available / (1024**3)) / recommended_workers))\n",
    "print(f\"Recommended Dask Workers: {recommended_workers}\")\n",
    "print(f\"Recommended Memory per Worker: {recommended_memory_per_worker} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c48bfa",
   "metadata": {},
   "source": [
    "<h3>Install required packages\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b948005b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CHECKING INSTALLED PACKAGES ===\n",
      "✓ dask: 2025.2.0\n",
      "✓ pandas: 2.2.3\n",
      "✓ numpy: 1.26.4\n",
      "✓ pyarrow: 19.0.1\n",
      "✓ psutil: 7.0.0\n",
      "✓ bokeh: 3.6.2\n",
      "\n",
      "=== PYTHON ENVIRONMENT ===\n",
      "Python executable: /opt/anaconda3/bin/python\n",
      "Current working directory: /Users/tusharjoshi/Desktop/ProjectWorkAll/Dissertation /CODE\n"
     ]
    }
   ],
   "source": [
    "# Check if required packages are installed\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "required_packages = [\n",
    "    'dask[complete]',\n",
    "    'pandas', \n",
    "    'numpy',\n",
    "    'pyarrow',  # For Parquet support\n",
    "    'psutil',   # For monitoring\n",
    "    'bokeh',    # For Dask dashboard\n",
    "]\n",
    "\n",
    "print(\"=== CHECKING INSTALLED PACKAGES ===\")\n",
    "for package in required_packages:\n",
    "    try:\n",
    "        if package == 'dask[complete]':\n",
    "            import dask\n",
    "            print(f\"✓ dask: {dask.__version__}\")\n",
    "        elif package == 'pandas':\n",
    "            import pandas as pd\n",
    "            print(f\"✓ pandas: {pd.__version__}\")\n",
    "        elif package == 'numpy':\n",
    "            import numpy as np\n",
    "            print(f\"✓ numpy: {np.__version__}\")\n",
    "        elif package == 'pyarrow':\n",
    "            import pyarrow as pa\n",
    "            print(f\"✓ pyarrow: {pa.__version__}\")\n",
    "        elif package == 'psutil':\n",
    "            import psutil\n",
    "            print(f\"✓ psutil: {psutil.__version__}\")\n",
    "        elif package == 'bokeh':\n",
    "            import bokeh\n",
    "            print(f\"✓ bokeh: {bokeh.__version__}\")\n",
    "    except ImportError:\n",
    "        print(f\"✗ {package}: NOT INSTALLED\")\n",
    "        print(f\"Install with: pip install {package}\")\n",
    "\n",
    "print(f\"\\n=== PYTHON ENVIRONMENT ===\")\n",
    "print(f\"Python executable: {sys.executable}\")\n",
    "print(f\"Current working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359c540c",
   "metadata": {},
   "source": [
    "<h3>Basic DASK import and version check\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7a8b1a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TESTING DASK IMPORTS ===\n",
      "Dask version: 2025.2.0\n",
      "All Dask components imported successfully\n",
      "\n",
      "=== MEMORY WARNING CHECK ===\n",
      "Available Memory: 1.6 GB\n",
      "WARNING: Very low memory available\n",
      "Consider closing other applications\n",
      "\n",
      "Ready to proceed with Dask cluster creation\n"
     ]
    }
   ],
   "source": [
    "import dask\n",
    "import dask.dataframe as dd\n",
    "import dask.bag as db\n",
    "from dask.distributed import Client, LocalCluster\n",
    "\n",
    "print(\"=== TESTING DASK IMPORTS ===\")\n",
    "print(f\"Dask version: {dask.__version__}\")\n",
    "print(\"All Dask components imported successfully\")\n",
    "\n",
    "print(\"\\n=== MEMORY WARNING CHECK ===\")\n",
    "import psutil\n",
    "memory = psutil.virtual_memory()\n",
    "available_gb = memory.available / (1024**3)\n",
    "print(f\"Available Memory: {available_gb:.1f} GB\")\n",
    "\n",
    "if available_gb < 2:\n",
    "    print(\"WARNING: Very low memory available\")\n",
    "    print(\"Consider closing other applications\")\n",
    "else:\n",
    "    print(\"Memory looks sufficient for basic testing\")\n",
    "\n",
    "print(\"\\nReady to proceed with Dask cluster creation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efc8a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CREATING DASK CLUSTER ===\n",
      "Cluster created successfully\n",
      "Client connected to cluster\n",
      "\n",
      "=== CLUSTER INFORMATION ===\n",
      "Dashboard link: http://127.0.0.1:8787/status\n",
      "Number of workers: 2\n",
      "\n",
      "=== TESTING COMPUTATION ===\n",
      "Test computation result: 10000.0\n",
      "\n",
      "=== SUCCESS ===\n",
      "Dask cluster is running and ready\n",
      "You can open the dashboard link in your browser\n",
      "Keep this running for the next steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-04 01:56:28,357 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:49414 (pid=1304) exceeded 95% memory budget. Restarting...\n",
      "2025-07-04 01:56:28,379 - distributed.nanny - WARNING - Restarting worker\n",
      "2025-07-04 01:56:28,654 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:49415 (pid=1303) exceeded 95% memory budget. Restarting...\n",
      "2025-07-04 01:56:28,674 - distributed.nanny - WARNING - Restarting worker\n",
      "2025-07-04 01:56:30,757 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:49577 (pid=1403) exceeded 95% memory budget. Restarting...\n",
      "2025-07-04 01:56:30,774 - distributed.nanny - WARNING - Restarting worker\n",
      "2025-07-04 01:56:32,254 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:49580 (pid=1404) exceeded 95% memory budget. Restarting...\n",
      "2025-07-04 01:56:32,272 - distributed.nanny - WARNING - Restarting worker\n",
      "2025-07-04 01:56:33,757 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:49586 (pid=1407) exceeded 95% memory budget. Restarting...\n",
      "2025-07-04 01:56:33,774 - distributed.scheduler - ERROR - Task ('flatten-from_sequence-process_single_file-to_dataframe-9b0f3775507126bc1fcd6829e69483d4', 0) marked as failed because 4 workers died while trying to run it\n",
      "2025-07-04 01:56:33,774 - distributed.scheduler - ERROR - Task ('flatten-from_sequence-process_single_file-to_dataframe-9b0f3775507126bc1fcd6829e69483d4', 1) marked as failed because 4 workers died while trying to run it\n",
      "2025-07-04 01:56:33,778 - distributed.nanny - WARNING - Restarting worker\n"
     ]
    }
   ],
   "source": [
    "from dask.distributed import Client, LocalCluster\n",
    "import dask\n",
    "\n",
    "print(\"=== CREATING DASK CLUSTER ===\")\n",
    "\n",
    "# Very conservative settings for low memory\n",
    "cluster = LocalCluster(\n",
    "    n_workers=2,           # Only 2 workers due to memory constraints\n",
    "    threads_per_worker=2,  # 2 threads each\n",
    "    memory_limit='500MB',  # Very conservative memory per worker\n",
    "    dashboard_address=':8787'\n",
    ")\n",
    "\n",
    "print(\"Cluster created successfully\")\n",
    "\n",
    "# Connect client\n",
    "client = Client(cluster)\n",
    "print(\"Client connected to cluster\")\n",
    "\n",
    "# Show cluster info\n",
    "print(f\"\\n=== CLUSTER INFORMATION ===\")\n",
    "print(f\"Dashboard link: {client.dashboard_link}\")\n",
    "print(f\"Number of workers: {len(client.scheduler_info()['workers'])}\")\n",
    "\n",
    "# Simple test computation\n",
    "print(f\"\\n=== TESTING COMPUTATION ===\")\n",
    "import dask.array as da\n",
    "\n",
    "# Small test array\n",
    "x = da.ones((100, 100), chunks=(50, 50))\n",
    "result = x.sum().compute()\n",
    "print(f\"Test computation result: {result}\")\n",
    "\n",
    "print(f\"\\n=== SUCCESS ===\")\n",
    "print(\"Dask cluster is running and ready\")\n",
    "print(\"You can open the dashboard link in your browser\")\n",
    "print(\"Keep this running for the next steps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdce646",
   "metadata": {},
   "source": [
    "<h3>Discovering all data files</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6ec20c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DISCOVERING DATA FILES ===\n",
      "2024 data directory: /Users/tusharjoshi/Desktop/ProjectWorkAll/Dissertation /DATA/2024\n",
      "2025 data directory: /Users/tusharjoshi/Desktop/ProjectWorkAll/Dissertation /DATA/2025\n",
      "\n",
      "=== DIRECTORY EXISTENCE CHECK ===\n",
      "2024 directory exists: True\n",
      "2025 directory exists: True\n",
      "Found 238 .gz files in 2024 directory\n",
      "Found 93 .gz files in 2025 directory\n",
      "\n",
      "=== FILE DISCOVERY RESULTS ===\n",
      "Total .gz files found: 331\n",
      "\n",
      "First few 2024 files:\n",
      "  2024-04-03-prusa.gz\n",
      "  2024-04-04-prusa.gz\n",
      "  2024-04-05-prusa.gz\n",
      "\n",
      "First few 2025 files:\n",
      "  2025-01-20-prusa.gz\n",
      "  2025-01-21-prusa.gz\n",
      "  2025-01-22-prusa.gz\n",
      "\n",
      "Total data size: 0.56 GB\n",
      "\n",
      "Ready to process 331 files\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=== DISCOVERING DATA FILES ===\")\n",
    "\n",
    "# Your specific data directories\n",
    "data_dir_2024 = Path(\"/Users/tusharjoshi/Desktop/ProjectWorkAll/Dissertation /DATA/2024\")\n",
    "data_dir_2025 = Path(\"/Users/tusharjoshi/Desktop/ProjectWorkAll/Dissertation /DATA/2025\")\n",
    "\n",
    "print(f\"2024 data directory: {data_dir_2024}\")\n",
    "print(f\"2025 data directory: {data_dir_2025}\")\n",
    "\n",
    "# Check if directories exist\n",
    "print(f\"\\n=== DIRECTORY EXISTENCE CHECK ===\")\n",
    "dir_2024_exists = data_dir_2024.exists()\n",
    "dir_2025_exists = data_dir_2025.exists()\n",
    "\n",
    "print(f\"2024 directory exists: {dir_2024_exists}\")\n",
    "print(f\"2025 directory exists: {dir_2025_exists}\")\n",
    "\n",
    "# Find all .gz files\n",
    "all_files = []\n",
    "\n",
    "if dir_2024_exists:\n",
    "    files_2024 = list(data_dir_2024.glob(\"*.gz\"))\n",
    "    print(f\"Found {len(files_2024)} .gz files in 2024 directory\")\n",
    "    all_files.extend(files_2024)\n",
    "\n",
    "if dir_2025_exists:\n",
    "    files_2025 = list(data_dir_2025.glob(\"*.gz\"))\n",
    "    print(f\"Found {len(files_2025)} .gz files in 2025 directory\")\n",
    "    all_files.extend(files_2025)\n",
    "\n",
    "print(f\"\\n=== FILE DISCOVERY RESULTS ===\")\n",
    "print(f\"Total .gz files found: {len(all_files)}\")\n",
    "\n",
    "if len(all_files) > 0:\n",
    "    # Show first few files from each year\n",
    "    print(f\"\\nFirst few 2024 files:\")\n",
    "    files_2024_sorted = sorted([f for f in all_files if \"2024\" in str(f)])[:3]\n",
    "    for file_path in files_2024_sorted:\n",
    "        print(f\"  {file_path.name}\")\n",
    "    \n",
    "    print(f\"\\nFirst few 2025 files:\")\n",
    "    files_2025_sorted = sorted([f for f in all_files if \"2025\" in str(f)])[:3]\n",
    "    for file_path in files_2025_sorted:\n",
    "        print(f\"  {file_path.name}\")\n",
    "    \n",
    "    # Show file size info\n",
    "    total_size = sum(f.stat().st_size for f in all_files if f.exists())\n",
    "    print(f\"\\nTotal data size: {total_size / (1024**3):.2f} GB\")\n",
    "    \n",
    "else:\n",
    "    print(\"No .gz files found in the specified directories\")\n",
    "\n",
    "# Store file paths for next step\n",
    "if len(all_files) > 0:\n",
    "    file_paths = [str(f) for f in all_files]\n",
    "    print(f\"\\nReady to process {len(file_paths)} files\")\n",
    "else:\n",
    "    print(\"Cannot proceed without data files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf57942c",
   "metadata": {},
   "source": [
    "<h2>Analysing File structure and Creating processing bathes</h2>\n",
    "<h3>Analyzes all your files, sorts them by date, calculates sizes, and creates processing batches of 20 files each for efficient processing.</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52a8bf34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ANALYZING FILE STRUCTURE ===\n",
      "Processing 331 files\n",
      "Successfully analyzed 331 files\n",
      "\n",
      "=== DATE RANGE ANALYSIS ===\n",
      "Date range: 2024-04-03 to 2025-04-29\n",
      "Average file size: 1.74 MB\n",
      "File size range: 0.00 MB to 3.00 MB\n",
      "\n",
      "=== CREATING PROCESSING BATCHES ===\n",
      "Created 17 processing batches\n",
      "Files per batch: 20 (except last batch may have fewer)\n",
      "\n",
      "=== BATCH PREVIEW ===\n",
      "Batch 1: 20 files, 37.3 MB, 2024-04-03 to 2024-05-02\n",
      "Batch 2: 20 files, 36.0 MB, 2024-05-03 to 2024-05-22\n",
      "Batch 3: 20 files, 26.7 MB, 2024-05-23 to 2024-06-11\n",
      "... and 14 more batches\n",
      "\n",
      "=== READY FOR PROCESSING ===\n",
      "Total batches to process: 17\n",
      "Estimated processing time: 34 minutes (2 min per batch)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"=== ANALYZING FILE STRUCTURE ===\")\n",
    "\n",
    "# Get all files again (from previous step)\n",
    "data_dir_2024 = Path(\"/Users/tusharjoshi/Desktop/ProjectWorkAll/Dissertation /DATA/2024\")\n",
    "data_dir_2025 = Path(\"/Users/tusharjoshi/Desktop/ProjectWorkAll/Dissertation /DATA/2025\")\n",
    "\n",
    "all_files = []\n",
    "all_files.extend(list(data_dir_2024.glob(\"*.gz\")))\n",
    "all_files.extend(list(data_dir_2025.glob(\"*.gz\")))\n",
    "\n",
    "print(f\"Processing {len(all_files)} files\")\n",
    "\n",
    "# Analyze file details\n",
    "file_details = []\n",
    "for file_path in all_files:\n",
    "    try:\n",
    "        # Extract date from filename\n",
    "        filename = file_path.name\n",
    "        date_part = filename.split('-prusa.gz')[0]  # Get YYYY-MM-DD part\n",
    "        file_date = datetime.strptime(date_part, \"%Y-%m-%d\")\n",
    "        file_size = file_path.stat().st_size\n",
    "        \n",
    "        file_details.append({\n",
    "            'path': str(file_path),\n",
    "            'filename': filename,\n",
    "            'date': file_date,\n",
    "            'size_mb': file_size / (1024**2),\n",
    "            'year': file_date.year,\n",
    "            'month': file_date.month\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path.name}: {e}\")\n",
    "\n",
    "print(f\"Successfully analyzed {len(file_details)} files\")\n",
    "\n",
    "# Sort by date\n",
    "file_details.sort(key=lambda x: x['date'])\n",
    "\n",
    "print(f\"\\n=== DATE RANGE ANALYSIS ===\")\n",
    "if file_details:\n",
    "    first_date = file_details[0]['date']\n",
    "    last_date = file_details[-1]['date']\n",
    "    print(f\"Date range: {first_date.strftime('%Y-%m-%d')} to {last_date.strftime('%Y-%m-%d')}\")\n",
    "    \n",
    "    # Show file size statistics\n",
    "    sizes = [f['size_mb'] for f in file_details]\n",
    "    avg_size = sum(sizes) / len(sizes)\n",
    "    min_size = min(sizes)\n",
    "    max_size = max(sizes)\n",
    "    \n",
    "    print(f\"Average file size: {avg_size:.2f} MB\")\n",
    "    print(f\"File size range: {min_size:.2f} MB to {max_size:.2f} MB\")\n",
    "\n",
    "# Create processing batches (groups of files)\n",
    "print(f\"\\n=== CREATING PROCESSING BATCHES ===\")\n",
    "\n",
    "batch_size = 20  # Process 20 files at a time\n",
    "batches = []\n",
    "\n",
    "for i in range(0, len(file_details), batch_size):\n",
    "    batch_files = file_details[i:i+batch_size]\n",
    "    batch_size_mb = sum(f['size_mb'] for f in batch_files)\n",
    "    \n",
    "    batches.append({\n",
    "        'batch_id': len(batches) + 1,\n",
    "        'files': batch_files,\n",
    "        'file_count': len(batch_files),\n",
    "        'total_size_mb': batch_size_mb,\n",
    "        'date_range': f\"{batch_files[0]['date'].strftime('%Y-%m-%d')} to {batch_files[-1]['date'].strftime('%Y-%m-%d')}\"\n",
    "    })\n",
    "\n",
    "print(f\"Created {len(batches)} processing batches\")\n",
    "print(f\"Files per batch: {batch_size} (except last batch may have fewer)\")\n",
    "\n",
    "# Show first few batches\n",
    "print(f\"\\n=== BATCH PREVIEW ===\")\n",
    "for i, batch in enumerate(batches[:3]):\n",
    "    print(f\"Batch {batch['batch_id']}: {batch['file_count']} files, {batch['total_size_mb']:.1f} MB, {batch['date_range']}\")\n",
    "\n",
    "if len(batches) > 3:\n",
    "    print(f\"... and {len(batches) - 3} more batches\")\n",
    "\n",
    "print(f\"\\n=== READY FOR PROCESSING ===\")\n",
    "print(f\"Total batches to process: {len(batches)}\")\n",
    "print(f\"Estimated processing time: {len(batches) * 2} minutes (2 min per batch)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf1389e",
   "metadata": {},
   "source": [
    "<h3>Processing first branch</h3>\n",
    "\n",
    "Takes first 3 files from your data <br>\n",
    "Reads each .gz file and extracts JSON records<br>\n",
    "Flattens the data (same as your original approach)<br>\n",
    "Creates a Dask DataFrame with all records<br>\n",
    "Shows basic info about the result<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f166f93d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PROCESSING FIRST BATCH ===\n",
      "Processing 3 files:\n",
      "  2024-04-03-prusa.gz\n",
      "  2024-04-04-prusa.gz\n",
      "  2024-04-05-prusa.gz\n",
      "Batch processed successfully\n",
      "DataFrame shape: (<dask_expr.expr.Scalar: expr=FromGraph(7c89dc5).size() // 15, dtype=int64>, 15)\n",
      "Columns: ['timestamp', 'printer_id', 'status', 'state', 'tempBed', 'targetBed', 'tempNozzle', 'targetNozzle', 'axisZ', 'axisX', 'axisY', 'flow', 'speed', 'fanHotend', 'fanPrint']\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import json\n",
    "import dask.bag as db\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=== PROCESSING FIRST BATCH ===\")\n",
    "\n",
    "# Get first batch of files\n",
    "data_dir_2024 = Path(\"/Users/tusharjoshi/Desktop/ProjectWorkAll/Dissertation /DATA/2024\")\n",
    "data_dir_2025 = Path(\"/Users/tusharjoshi/Desktop/ProjectWorkAll/Dissertation /DATA/2025\")\n",
    "\n",
    "all_files = []\n",
    "all_files.extend(list(data_dir_2024.glob(\"*.gz\")))\n",
    "all_files.extend(list(data_dir_2025.glob(\"*.gz\")))\n",
    "all_files.sort()\n",
    "\n",
    "# Take first 3 files for testing\n",
    "first_batch = [str(f) for f in all_files[:3]]\n",
    "\n",
    "print(f\"Processing {len(first_batch)} files:\")\n",
    "for filepath in first_batch:\n",
    "    print(f\"  {Path(filepath).name}\")\n",
    "\n",
    "# Process batch with Dask\n",
    "bag = db.from_sequence(first_batch, npartitions=2)\n",
    "\n",
    "def process_single_file(filepath):\n",
    "    records = []\n",
    "    with gzip.open(filepath, 'rt', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                record = json.loads(line.strip())\n",
    "                # Flatten the record\n",
    "                flat_record = {\n",
    "                    'timestamp': record.get('date'),\n",
    "                    'printer_id': record.get('id'),\n",
    "                    'status': record.get('check')\n",
    "                }\n",
    "                # Add data fields\n",
    "                if record.get('data'):\n",
    "                    flat_record.update(record['data'])\n",
    "                records.append(flat_record)\n",
    "    return records\n",
    "\n",
    "# Process files and flatten\n",
    "records_bag = bag.map(process_single_file).flatten()\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = records_bag.to_dataframe()\n",
    "\n",
    "print(f\"Batch processed successfully\")\n",
    "print(f\"DataFrame shape: {df.shape}\")\n",
    "print(f\"Columns: {list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c3a7e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== GETTING DETAILED BATCH INFORMATION ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-04 01:56:28,036 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 356.68 MiB -- Worker memory limit: 476.84 MiB\n",
      "2025-07-04 01:56:28,136 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 358.75 MiB -- Worker memory limit: 476.84 MiB\n",
      "2025-07-04 01:56:28,272 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 385.30 MiB -- Worker memory limit: 476.84 MiB\n",
      "2025-07-04 01:56:30,283 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 352.44 MiB -- Worker memory limit: 476.84 MiB\n",
      "2025-07-04 01:56:30,512 - distributed.worker.memory - WARNING - Worker is at 84% memory usage. Pausing worker.  Process memory: 403.83 MiB -- Worker memory limit: 476.84 MiB\n",
      "2025-07-04 01:56:31,718 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 343.48 MiB -- Worker memory limit: 476.84 MiB\n",
      "2025-07-04 01:56:31,948 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 396.77 MiB -- Worker memory limit: 476.84 MiB\n",
      "2025-07-04 01:56:33,230 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 346.52 MiB -- Worker memory limit: 476.84 MiB\n",
      "2025-07-04 01:56:33,457 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 397.71 MiB -- Worker memory limit: 476.84 MiB\n"
     ]
    },
    {
     "ename": "KilledWorker",
     "evalue": "Attempted to run task ('flatten-from_sequence-process_single_file-to_dataframe-9b0f3775507126bc1fcd6829e69483d4', 1) on 4 different workers, but all those workers died while running it. The last worker that attempt to run the task was tcp://127.0.0.1:49586. Inspecting worker logs is often a good next step to diagnose what went wrong. For more information see https://distributed.dask.org/en/stable/killed.html.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKilledWorker\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=== GETTING DETAILED BATCH INFORMATION ===\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Get actual row count\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m total_rows = \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTotal records in batch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_rows\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Get first few rows to see the data\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.11/site-packages/dask/dataframe/dask_expr/_collection.py:389\u001b[39m, in \u001b[36mFrameBase.__len__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    388\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__len__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnew_collection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mLen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.11/site-packages/dask/dataframe/dask_expr/_collection.py:489\u001b[39m, in \u001b[36mFrameBase.compute\u001b[39m\u001b[34m(self, fuse, concatenate, **kwargs)\u001b[39m\n\u001b[32m    487\u001b[39m     out = out.repartition(npartitions=\u001b[32m1\u001b[39m)\n\u001b[32m    488\u001b[39m out = out.optimize(fuse=fuse)\n\u001b[32m--> \u001b[39m\u001b[32m489\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDaskMethodsMixin\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.11/site-packages/dask/base.py:374\u001b[39m, in \u001b[36mDaskMethodsMixin.compute\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m    350\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute\u001b[39m(\u001b[38;5;28mself\u001b[39m, **kwargs):\n\u001b[32m    351\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Compute this dask collection\u001b[39;00m\n\u001b[32m    352\u001b[39m \n\u001b[32m    353\u001b[39m \u001b[33;03m    This turns a lazy Dask collection into its in-memory equivalent.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    372\u001b[39m \u001b[33;03m    dask.compute\u001b[39;00m\n\u001b[32m    373\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m374\u001b[39m     (result,) = \u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraverse\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    375\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.11/site-packages/dask/base.py:662\u001b[39m, in \u001b[36mcompute\u001b[39m\u001b[34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[39m\n\u001b[32m    659\u001b[39m     postcomputes.append(x.__dask_postcompute__())\n\u001b[32m    661\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m shorten_traceback():\n\u001b[32m--> \u001b[39m\u001b[32m662\u001b[39m     results = \u001b[43mschedule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdsk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    664\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m repack([f(r, *a) \u001b[38;5;28;01mfor\u001b[39;00m r, (f, a) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(results, postcomputes)])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.11/site-packages/distributed/client.py:2426\u001b[39m, in \u001b[36mClient._gather\u001b[39m\u001b[34m(self, futures, errors, direct, local_worker)\u001b[39m\n\u001b[32m   2424\u001b[39m     exception = st.exception\n\u001b[32m   2425\u001b[39m     traceback = st.traceback\n\u001b[32m-> \u001b[39m\u001b[32m2426\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exception.with_traceback(traceback)\n\u001b[32m   2427\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m errors == \u001b[33m\"\u001b[39m\u001b[33mskip\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   2428\u001b[39m     bad_keys.add(key)\n",
      "\u001b[31mKilledWorker\u001b[39m: Attempted to run task ('flatten-from_sequence-process_single_file-to_dataframe-9b0f3775507126bc1fcd6829e69483d4', 1) on 4 different workers, but all those workers died while running it. The last worker that attempt to run the task was tcp://127.0.0.1:49586. Inspecting worker logs is often a good next step to diagnose what went wrong. For more information see https://distributed.dask.org/en/stable/killed.html."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-04 02:14:18,040 - tornado.application - ERROR - Exception in callback <bound method SystemMonitor.update of <SystemMonitor: cpu: 0 memory: 98 MB fds: 24>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/tusharjoshi/.local/lib/python3.11/site-packages/tornado/ioloop.py\", line 937, in _run\n",
      "    val = self.callback()\n",
      "          ^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/distributed/system_monitor.py\", line 168, in update\n",
      "    net_ioc = psutil.net_io_counters()\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tusharjoshi/.local/lib/python3.11/site-packages/psutil/__init__.py\", line 2148, in net_io_counters\n",
      "    rawdict = _psplatform.net_io_counters()\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "OSError: [Errno 12] Cannot allocate memory\n",
      "2025-07-04 04:00:24,040 - tornado.application - ERROR - Exception in callback <bound method SystemMonitor.update of <SystemMonitor: cpu: 0 memory: 62 MB fds: 24>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/tusharjoshi/.local/lib/python3.11/site-packages/tornado/ioloop.py\", line 937, in _run\n",
      "    val = self.callback()\n",
      "          ^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/distributed/system_monitor.py\", line 168, in update\n",
      "    net_ioc = psutil.net_io_counters()\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tusharjoshi/.local/lib/python3.11/site-packages/psutil/__init__.py\", line 2148, in net_io_counters\n",
      "    rawdict = _psplatform.net_io_counters()\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "OSError: [Errno 12] Cannot allocate memory\n"
     ]
    }
   ],
   "source": [
    "print(\"=== GETTING DETAILED BATCH INFORMATION ===\")\n",
    "\n",
    "# Get actual row count\n",
    "total_rows = len(df)\n",
    "print(f\"Total records in batch: {total_rows}\")\n",
    "\n",
    "# Get first few rows to see the data\n",
    "print(f\"\\n=== SAMPLE DATA ===\")\n",
    "sample_data = df.head()\n",
    "print(f\"First 5 rows:\")\n",
    "print(sample_data)\n",
    "\n",
    "print(f\"\\n=== BATCH PROCESSING SUCCESS ===\")\n",
    "print(f\"✓ Successfully processed 3 files\")\n",
    "print(f\"✓ Found {total_rows} total records\") \n",
    "print(f\"✓ All expected columns present\")\n",
    "print(f\"✓ Data structure matches previous work\")\n",
    "print(f\"✓ Ready to process all 331 files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5d415b",
   "metadata": {},
   "source": [
    "<h3>Due to memory constraints, instead of running making batches we will process one file at a time into dask</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9afbb60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== RESTARTING DASK WITH MEMORY-AWARE SETTINGS ===\n",
      "Dask cluster restarted with proper memory management\n",
      "Dashboard: http://127.0.0.1:8787/status\n"
     ]
    }
   ],
   "source": [
    "# Close current cluster\n",
    "client.close()\n",
    "cluster.close()\n",
    "\n",
    "print(\"=== RESTARTING DASK WITH MEMORY-AWARE SETTINGS ===\")\n",
    "\n",
    "from dask.distributed import Client, LocalCluster\n",
    "import dask\n",
    "\n",
    "# Configure Dask for your system\n",
    "dask.config.set({\n",
    "    'distributed.worker.memory.target': 0.6,\n",
    "    'distributed.worker.memory.spill': 0.7,\n",
    "    'distributed.worker.memory.pause': 0.8,\n",
    "    'distributed.worker.memory.terminate': 0.9\n",
    "})\n",
    "\n",
    "cluster = LocalCluster(\n",
    "    n_workers=1,\n",
    "    threads_per_worker=2,\n",
    "    memory_limit='800MB',\n",
    "    dashboard_address=':8787'\n",
    ")\n",
    "\n",
    "client = Client(cluster)\n",
    "print(\"Dask cluster restarted with proper memory management\")\n",
    "print(f\"Dashboard: {client.dashboard_link}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461ef3e3",
   "metadata": {},
   "source": [
    "<h3>Test Single File processing</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a378c953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CORRECT APPROACH: NATURAL MISSING VALUE HANDLING ===\n",
      "✅ Processing with natural type handling\n",
      "Columns: ['timestamp', 'printer_id', 'status', 'state', 'tempBed', 'targetBed', 'tempNozzle', 'targetNozzle', 'axisZ', 'axisX', 'axisY', 'flow', 'speed', 'fanHotend', 'fanPrint']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-04 04:24:01,709 - distributed.worker - ERROR - Compute Failed\n",
      "Key:       ('flatten-from_sequence-process_single_file_natural-to_dataframe-e4c65c0fc795dfa9d8644dbc21ad6817', 0)\n",
      "State:     executing\n",
      "Task:  <Task ('flatten-from_sequence-process_single_file_natural-to_dataframe-e4c65c0fc795dfa9d8644dbc21ad6817', 0) _execute_subgraph(...)>\n",
      "Exception: 'IntCastingNaNError(\"Cannot convert non-finite values (NA or inf) to integer: Error while type casting for column \\'targetBed\\'\")'\n",
      "Traceback: '  File \"/opt/anaconda3/lib/python3.11/site-packages/dask/bag/core.py\", line 2616, in to_dataframe\\n    return res.astype(dtypes, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/opt/anaconda3/lib/python3.11/site-packages/pandas/core/generic.py\", line 6620, in astype\\n    res_col = col.astype(dtype=cdt, copy=copy, errors=errors)\\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/opt/anaconda3/lib/python3.11/site-packages/pandas/core/generic.py\", line 6643, in astype\\n    new_data = self._mgr.astype(dtype=dtype, copy=copy, errors=errors)\\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/opt/anaconda3/lib/python3.11/site-packages/pandas/core/internals/managers.py\", line 430, in astype\\n    return self.apply(\\n           ^^^^^^^^^^^\\n  File \"/opt/anaconda3/lib/python3.11/site-packages/pandas/core/internals/managers.py\", line 363, in apply\\n    applied = getattr(b, f)(**kwargs)\\n              ^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/opt/anaconda3/lib/python3.11/site-packages/pandas/core/internals/blocks.py\", line 758, in astype\\n    new_values = astype_array_safe(values, dtype, copy=copy, errors=errors)\\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/opt/anaconda3/lib/python3.11/site-packages/pandas/core/dtypes/astype.py\", line 237, in astype_array_safe\\n    new_values = astype_array(values, dtype, copy=copy)\\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/opt/anaconda3/lib/python3.11/site-packages/pandas/core/dtypes/astype.py\", line 182, in astype_array\\n    values = _astype_nansafe(values, dtype, copy=copy)\\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/opt/anaconda3/lib/python3.11/site-packages/pandas/core/dtypes/astype.py\", line 101, in _astype_nansafe\\n    return _astype_float_to_int_nansafe(arr, dtype, copy)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/opt/anaconda3/lib/python3.11/site-packages/pandas/core/dtypes/astype.py\", line 145, in _astype_float_to_int_nansafe\\n    raise IntCastingNaNError(\\n'\n",
      "\n"
     ]
    },
    {
     "ename": "IntCastingNaNError",
     "evalue": "Cannot convert non-finite values (NA or inf) to integer: Error while type casting for column 'targetBed'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIntCastingNaNError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 34\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mColumns: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(df_single.columns)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# Get sample\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m sample = \u001b[43mdf_single\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhead\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSample data:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     36\u001b[39m \u001b[38;5;28mprint\u001b[39m(sample)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.11/site-packages/dask/dataframe/dask_expr/_collection.py:711\u001b[39m, in \u001b[36mFrameBase.head\u001b[39m\u001b[34m(self, n, npartitions, compute)\u001b[39m\n\u001b[32m    709\u001b[39m out = new_collection(expr.Head(\u001b[38;5;28mself\u001b[39m, n=n, npartitions=npartitions))\n\u001b[32m    710\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m compute:\n\u001b[32m--> \u001b[39m\u001b[32m711\u001b[39m     out = \u001b[43mout\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    712\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.11/site-packages/dask/dataframe/dask_expr/_collection.py:489\u001b[39m, in \u001b[36mFrameBase.compute\u001b[39m\u001b[34m(self, fuse, concatenate, **kwargs)\u001b[39m\n\u001b[32m    487\u001b[39m     out = out.repartition(npartitions=\u001b[32m1\u001b[39m)\n\u001b[32m    488\u001b[39m out = out.optimize(fuse=fuse)\n\u001b[32m--> \u001b[39m\u001b[32m489\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDaskMethodsMixin\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.11/site-packages/dask/base.py:374\u001b[39m, in \u001b[36mDaskMethodsMixin.compute\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m    350\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute\u001b[39m(\u001b[38;5;28mself\u001b[39m, **kwargs):\n\u001b[32m    351\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Compute this dask collection\u001b[39;00m\n\u001b[32m    352\u001b[39m \n\u001b[32m    353\u001b[39m \u001b[33;03m    This turns a lazy Dask collection into its in-memory equivalent.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    372\u001b[39m \u001b[33;03m    dask.compute\u001b[39;00m\n\u001b[32m    373\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m374\u001b[39m     (result,) = \u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraverse\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    375\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.11/site-packages/dask/base.py:662\u001b[39m, in \u001b[36mcompute\u001b[39m\u001b[34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[39m\n\u001b[32m    659\u001b[39m     postcomputes.append(x.__dask_postcompute__())\n\u001b[32m    661\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m shorten_traceback():\n\u001b[32m--> \u001b[39m\u001b[32m662\u001b[39m     results = \u001b[43mschedule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdsk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    664\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m repack([f(r, *a) \u001b[38;5;28;01mfor\u001b[39;00m r, (f, a) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(results, postcomputes)])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.11/site-packages/dask/bag/core.py:2616\u001b[39m, in \u001b[36mto_dataframe\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   2614\u001b[39m kwargs = {} \u001b[38;5;28;01mif\u001b[39;00m Version(pd.__version__).major >= \u001b[32m3\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33mcopy\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m}\n\u001b[32m   2615\u001b[39m res = pd.DataFrame(seq, columns=\u001b[38;5;28mlist\u001b[39m(columns))\n\u001b[32m-> \u001b[39m\u001b[32m2616\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m res.astype(dtypes, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/dtypes/astype.py:145\u001b[39m, in \u001b[36m_astype_float_to_int_nansafe\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    141\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    142\u001b[39m \u001b[33;03mastype with a check preventing converting NaN to an meaningless integer value.\u001b[39;00m\n\u001b[32m    143\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    144\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np.isfinite(values).all():\n\u001b[32m--> \u001b[39m\u001b[32m145\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m IntCastingNaNError(\n\u001b[32m    146\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCannot convert non-finite values (NA or inf) to integer\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    147\u001b[39m     )\n\u001b[32m    148\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m dtype.kind == \u001b[33m\"\u001b[39m\u001b[33mu\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    149\u001b[39m     \u001b[38;5;66;03m# GH#45151\u001b[39;00m\n\u001b[32m    150\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (values >= \u001b[32m0\u001b[39m).all():\n",
      "\u001b[31mIntCastingNaNError\u001b[39m: Cannot convert non-finite values (NA or inf) to integer: Error while type casting for column 'targetBed'"
     ]
    }
   ],
   "source": [
    "print(\"=== CORRECT APPROACH: NATURAL MISSING VALUE HANDLING ===\")\n",
    "\n",
    "def process_single_file_natural(filepath):\n",
    "    records = []\n",
    "    with gzip.open(filepath, 'rt', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                try:\n",
    "                    record = json.loads(line.strip())\n",
    "                    flat_record = {\n",
    "                        'timestamp': record.get('date'),\n",
    "                        'printer_id': record.get('id'), \n",
    "                        'status': record.get('check')\n",
    "                    }\n",
    "                    # Add data fields as-is, let Dask handle types\n",
    "                    if record.get('data'):\n",
    "                        flat_record.update(record['data'])\n",
    "                    records.append(flat_record)\n",
    "                except:\n",
    "                    continue\n",
    "    return records\n",
    "\n",
    "# Process with natural type inference\n",
    "bag = db.from_sequence([single_file_path], npartitions=1)\n",
    "records_bag = bag.map(process_single_file_natural).flatten()\n",
    "\n",
    "# Convert to dataframe WITHOUT forcing types\n",
    "df_single = records_bag.to_dataframe(meta=None)  # Let Dask figure out types\n",
    "\n",
    "print(f\"✅ Processing with natural type handling\")\n",
    "print(f\"Columns: {list(df_single.columns)}\")\n",
    "\n",
    "# Get sample\n",
    "sample = df_single.head(5)\n",
    "print(f\"Sample data:\")\n",
    "print(sample)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
