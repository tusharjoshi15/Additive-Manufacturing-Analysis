{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37120f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete. Ready to define schema.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import polars as pl\n",
    "from tableauhyperapi import HyperProcess, Connection, TableDefinition, SqlType, Inserter, CreateMode, TableName\n",
    "\n",
    "# --- Configuration ---\n",
    "# IMPORTANT: Update this path to the actual folder containing your Parquet files.\n",
    "parquet_folder = \"/Users/tusharjoshi/Desktop/ProjectWorkAll/Dissertation /processed_float\" \n",
    "\n",
    "# This is where your new .hyper file will be created.\n",
    "output_hyper_file = \"/Users/tusharjoshi/Desktop/ProjectWorkAll/Dissertation /new_printer_hyper_file.hyper\" \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51812e1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table schema defined. Ready to list Parquet files.\n"
     ]
    }
   ],
   "source": [
    "# --- Define the Table Schema and Name ---\n",
    "# The table will be named \"PrinterData\" within the \"Extract\" schema.\n",
    "\n",
    "table_name = TableName(\"Extract\", \"PrinterData\")\n",
    "\n",
    "# Define each column's name and its data type for the Hyper file.\n",
    "# We're explicitly setting 'date' to SqlType.timestamp() for proper date/time handling.\n",
    "\n",
    "printer_table_definition = TableDefinition(table_name)\n",
    "printer_table_definition.add_column(\"check\", SqlType.text())\n",
    "printer_table_definition.add_column(\"date\", SqlType.timestamp()) \n",
    "printer_table_definition.add_column(\"id\", SqlType.text())\n",
    "printer_table_definition.add_column(\"state\", SqlType.text())\n",
    "printer_table_definition.add_column(\"tempBed\", SqlType.double())\n",
    "printer_table_definition.add_column(\"targetBed\", SqlType.double())\n",
    "printer_table_definition.add_column(\"tempNozzle\", SqlType.double())\n",
    "printer_table_definition.add_column(\"targetNozzle\", SqlType.double())\n",
    "printer_table_definition.add_column(\"axisZ\", SqlType.double())\n",
    "printer_table_definition.add_column(\"axisX\", SqlType.double())\n",
    "printer_table_definition.add_column(\"axisY\", SqlType.double())\n",
    "printer_table_definition.add_column(\"flow\", SqlType.double())\n",
    "printer_table_definition.add_column(\"speed\", SqlType.double())\n",
    "printer_table_definition.add_column(\"fanHotend\", SqlType.double())\n",
    "printer_table_definition.add_column(\"fanPrint\", SqlType.double())\n",
    "\n",
    "print(\"Table schema defined. Ready to list Parquet files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49c90a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 34 parquet files to export.\n"
     ]
    }
   ],
   "source": [
    "# --- List Parquet Files ---\n",
    "all_parquet_files = sorted([\n",
    "    os.path.join(parquet_folder, f)\n",
    "    for f in os.listdir(parquet_folder)\n",
    "    if f.endswith(\".parquet\")\n",
    "])\n",
    "print(f\"Found {len(all_parquet_files)} parquet files to export.\")\n",
    "\n",
    "if not all_parquet_files:\n",
    "    print(\"No parquet files found in the specified folder. Nothing to export.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17e0cee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created table '\"Extract\".\"PrinterData\"' in '/Users/tusharjoshi/Desktop/ProjectWorkAll/Dissertation /new_printer_hyper_file.hyper'. Starting data insertion...\n"
     ]
    }
   ],
   "source": [
    "# --- Start Hyper Process and Create File ---\n",
    "# This block will only run if parquet files were found.\n",
    "if all_parquet_files:\n",
    "    # Start the Hyper process. (Using telemetry=0 for compatibility with your version)\n",
    "    hyper_process = HyperProcess(telemetry=0)\n",
    "    connection = None # Initialize connection to None\n",
    "\n",
    "    try:\n",
    "        # Connect to the Hyper file. CreateMode.CREATE_AND_REPLACE will create a new file\n",
    "        # or overwrite an existing one, ensuring a clean start.\n",
    "        connection = Connection(hyper_process.endpoint, output_hyper_file, CreateMode.CREATE_AND_REPLACE)\n",
    "        \n",
    "        # Create the schema (e.g., \"Extract\") if it doesn't exist\n",
    "        connection.catalog.create_schema(table_name.schema_name)\n",
    "        # Create the table using the defined schema\n",
    "        connection.catalog.create_table(printer_table_definition)\n",
    "        print(f\"Created table '{table_name}' in '{output_hyper_file}'. Starting data insertion...\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during Hyper process or connection setup: {e}\")\n",
    "        # Ensure resources are closed even if an error occurs here\n",
    "        if connection:\n",
    "            connection.close()\n",
    "        hyper_process.close()\n",
    "        # Re-raise the exception to stop execution if setup failed\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b5bb552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing batch 1/34: batch_001.parquet\n",
      "  Inserted 3331416 rows. Total inserted: 3331416\n",
      "  Processing batch 2/34: batch_002.parquet\n",
      "  Inserted 2805908 rows. Total inserted: 6137324\n",
      "  Processing batch 3/34: batch_003.parquet\n",
      "  Inserted 3005163 rows. Total inserted: 9142487\n",
      "  Processing batch 4/34: batch_004.parquet\n",
      "  Inserted 3005002 rows. Total inserted: 12147489\n",
      "  Processing batch 5/34: batch_005.parquet\n",
      "  Inserted 2223424 rows. Total inserted: 14370913\n",
      "  Processing batch 6/34: batch_006.parquet\n",
      "  Inserted 2292976 rows. Total inserted: 16663889\n",
      "  Processing batch 7/34: batch_007.parquet\n",
      "  Inserted 3004799 rows. Total inserted: 19668688\n",
      "  Processing batch 8/34: batch_008.parquet\n",
      "  Inserted 3004960 rows. Total inserted: 22673648\n",
      "  Processing batch 9/34: batch_009.parquet\n",
      "  Inserted 3005002 rows. Total inserted: 25678650\n",
      "  Processing batch 10/34: batch_010.parquet\n",
      "  Inserted 3005135 rows. Total inserted: 28683785\n",
      "  Processing batch 11/34: batch_011.parquet\n",
      "  Inserted 2150302 rows. Total inserted: 30834087\n",
      "  Processing batch 12/34: batch_012.parquet\n",
      "  Inserted 2230298 rows. Total inserted: 33064385\n",
      "  Processing batch 13/34: batch_013.parquet\n",
      "  Inserted 3004981 rows. Total inserted: 36069366\n",
      "  Processing batch 14/34: batch_014.parquet\n",
      "  Inserted 3004567 rows. Total inserted: 39073933\n",
      "  Processing batch 15/34: batch_015.parquet\n",
      "  Inserted 3005184 rows. Total inserted: 42079117\n",
      "  Processing batch 16/34: batch_016.parquet\n",
      "  Inserted 2673293 rows. Total inserted: 44752410\n",
      "  Processing batch 17/34: batch_017.parquet\n",
      "  Inserted 2791481 rows. Total inserted: 47543891\n",
      "  Processing batch 18/34: batch_018.parquet\n",
      "  Inserted 2748438 rows. Total inserted: 50292329\n",
      "  Processing batch 19/34: batch_019.parquet\n",
      "  Inserted 3013772 rows. Total inserted: 53306101\n",
      "  Processing batch 20/34: batch_020.parquet\n",
      "  Inserted 3003581 rows. Total inserted: 56309682\n",
      "  Processing batch 21/34: batch_021.parquet\n",
      "  Inserted 2892316 rows. Total inserted: 59201998\n",
      "  Processing batch 22/34: batch_022.parquet\n",
      "  Inserted 3005730 rows. Total inserted: 62207728\n",
      "  Processing batch 23/34: batch_023.parquet\n",
      "  Inserted 2946755 rows. Total inserted: 65154483\n",
      "  Processing batch 24/34: batch_024.parquet\n",
      "  Inserted 2828046 rows. Total inserted: 67982529\n",
      "  Processing batch 25/34: batch_025.parquet\n",
      "  Inserted 2810412 rows. Total inserted: 70792941\n",
      "  Processing batch 26/34: batch_026.parquet\n",
      "  Inserted 3444748 rows. Total inserted: 74237689\n",
      "  Processing batch 27/34: batch_027.parquet\n",
      "  Inserted 3444792 rows. Total inserted: 77682481\n",
      "  Processing batch 28/34: batch_028.parquet\n",
      "  Inserted 3444896 rows. Total inserted: 81127377\n",
      "  Processing batch 29/34: batch_029.parquet\n",
      "  Inserted 3160644 rows. Total inserted: 84288021\n",
      "  Processing batch 30/34: batch_030.parquet\n",
      "  Inserted 2583156 rows. Total inserted: 86871177\n",
      "  Processing batch 31/34: batch_031.parquet\n",
      "  Inserted 2572740 rows. Total inserted: 89443917\n",
      "  Processing batch 32/34: batch_032.parquet\n",
      "  Inserted 2583606 rows. Total inserted: 92027523\n",
      "  Processing batch 33/34: batch_033.parquet\n",
      "  Inserted 2583780 rows. Total inserted: 94611303\n",
      "  Processing batch 34/34: batch_034.parquet\n",
      "  Inserted 258396 rows. Total inserted: 94869699\n",
      "\n",
      "Successfully inserted 94869699 rows to '/Users/tusharjoshi/Desktop/ProjectWorkAll/Dissertation /new_printer_hyper_file.hyper'.\n"
     ]
    }
   ],
   "source": [
    "# --- Insert Data from Parquet Files ---\n",
    "# This block assumes the previous block ran successfully and 'connection' is open.\n",
    "if all_parquet_files:\n",
    "    total_rows_inserted = 0\n",
    "    for i, parquet_file in enumerate(all_parquet_files):\n",
    "        print(f\"  Processing batch {i+1}/{len(all_parquet_files)}: {os.path.basename(parquet_file)}\")\n",
    "\n",
    "        # Read a single Parquet file into a Polars DataFrame\n",
    "        df = pl.read_parquet(parquet_file)\n",
    "\n",
    "        # Prepare rows for insertion.\n",
    "        # It's important that the order of data in 'row_data' matches the column order\n",
    "        # defined in 'printer_table_definition'.\n",
    "        rows_to_insert = []\n",
    "        column_names_in_order = [col.name for col in printer_table_definition.columns]\n",
    "        for row_dict in df.to_dicts():\n",
    "            ordered_row = tuple(row_dict.get(col_name) for col_name in column_names_in_order)\n",
    "            rows_to_insert.append(ordered_row)\n",
    "\n",
    "        # Use Inserter to add rows efficiently.\n",
    "        # The 'with Inserter' block ensures data is flushed.\n",
    "        with Inserter(connection, printer_table_definition) as inserter:\n",
    "            inserter.add_rows(rows_to_insert)\n",
    "        \n",
    "        total_rows_inserted += len(rows_to_insert)\n",
    "        print(f\"  Inserted {len(rows_to_insert)} rows. Total inserted: {total_rows_inserted}\")\n",
    "\n",
    "    print(f\"\\nSuccessfully inserted {total_rows_inserted} rows to '{output_hyper_file}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25743c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyper connection closed.\n",
      "Hyper process closed.\n",
      "\n",
      "Export process finished.\n"
     ]
    }
   ],
   "source": [
    "# --- Close Connection and Hyper Process ---\n",
    "# This ensures the .hyper file is properly finalized.\n",
    "if connection:\n",
    "    connection.close()\n",
    "    print(\"Hyper connection closed.\")\n",
    "if hyper_process:\n",
    "    hyper_process.close()\n",
    "    print(\"Hyper process closed.\")\n",
    "\n",
    "print(\"\\nExport process finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fde4aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to export a small sample from: batch_001.parquet\n",
      "Created table '\"Extract\".\"PrinterData\"' in '/Users/tusharjoshi/Desktop/ProjectWorkAll/Dissertation /test_small_printer_data.hyper'. Starting data insertion...\n",
      "  Inserted 100 rows into '/Users/tusharjoshi/Desktop/ProjectWorkAll/Dissertation /test_small_printer_data.hyper'.\n",
      "Hyper connection closed.\n",
      "Hyper process closed.\n",
      "\n",
      "Small test export process finished.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import polars as pl\n",
    "from tableauhyperapi import HyperProcess, Connection, TableDefinition, SqlType, Inserter, CreateMode, TableName\n",
    "\n",
    "# --- Configuration ---\n",
    "# IMPORTANT: Update this path to the actual folder containing your Parquet files.\n",
    "parquet_folder = \"/Users/tusharjoshi/Desktop/ProjectWorkAll/Dissertation /processed_float\" \n",
    "\n",
    "# This is where your new SMALL .hyper file will be created.\n",
    "output_small_hyper_file = \"/Users/tusharjoshi/Desktop/ProjectWorkAll/Dissertation /test_small_printer_data.hyper\" \n",
    "\n",
    "# --- Define the Table Schema and Name (same as before) ---\n",
    "table_name = TableName(\"Extract\", \"PrinterData\")\n",
    "\n",
    "printer_table_definition = TableDefinition(table_name)\n",
    "printer_table_definition.add_column(\"check\", SqlType.text())\n",
    "printer_table_definition.add_column(\"date\", SqlType.timestamp()) \n",
    "printer_table_definition.add_column(\"id\", SqlType.text())\n",
    "printer_table_definition.add_column(\"state\", SqlType.text())\n",
    "printer_table_definition.add_column(\"tempBed\", SqlType.double())\n",
    "printer_table_definition.add_column(\"targetBed\", SqlType.double())\n",
    "printer_table_definition.add_column(\"tempNozzle\", SqlType.double())\n",
    "printer_table_definition.add_column(\"targetNozzle\", SqlType.double())\n",
    "printer_table_definition.add_column(\"axisZ\", SqlType.double())\n",
    "printer_table_definition.add_column(\"axisX\", SqlType.double())\n",
    "printer_table_definition.add_column(\"axisY\", SqlType.double())\n",
    "printer_table_definition.add_column(\"flow\", SqlType.double())\n",
    "printer_table_definition.add_column(\"speed\", SqlType.double())\n",
    "printer_table_definition.add_column(\"fanHotend\", SqlType.double())\n",
    "printer_table_definition.add_column(\"fanPrint\", SqlType.double())\n",
    "\n",
    "# --- Main Export Process for a small test file ---\n",
    "hyper_process = None\n",
    "connection = None\n",
    "try:\n",
    "    # Get just the first parquet file for a small test\n",
    "    all_parquet_files = sorted([\n",
    "        os.path.join(parquet_folder, f)\n",
    "        for f in os.listdir(parquet_folder)\n",
    "        if f.endswith(\".parquet\")\n",
    "    ])\n",
    "    \n",
    "    if not all_parquet_files:\n",
    "        print(\"No parquet files found. Cannot perform test export.\")\n",
    "    else:\n",
    "        first_parquet_file = all_parquet_files[0]\n",
    "        print(f\"Attempting to export a small sample from: {os.path.basename(first_parquet_file)}\")\n",
    "\n",
    "        # Start the Hyper process\n",
    "        hyper_process = HyperProcess(telemetry=0) \n",
    "\n",
    "        # Connect to the Hyper file (create/replace)\n",
    "        connection = Connection(hyper_process.endpoint, output_small_hyper_file, CreateMode.CREATE_AND_REPLACE)\n",
    "        \n",
    "        # Create schema and table\n",
    "        connection.catalog.create_schema(table_name.schema_name)\n",
    "        connection.catalog.create_table(printer_table_definition)\n",
    "        print(f\"Created table '{table_name}' in '{output_small_hyper_file}'. Starting data insertion...\")\n",
    "\n",
    "        # Read only a small portion of the first parquet file\n",
    "        df_small = pl.read_parquet(first_parquet_file).head(100) # Read first 100 rows\n",
    "        \n",
    "        rows_to_insert = []\n",
    "        column_names_in_order = [col.name for col in printer_table_definition.columns]\n",
    "        for row_dict in df_small.to_dicts():\n",
    "            ordered_row = tuple(row_dict.get(col_name) for col_name in column_names_in_order)\n",
    "            rows_to_insert.append(ordered_row)\n",
    "\n",
    "        with Inserter(connection, printer_table_definition) as inserter:\n",
    "            inserter.add_rows(rows_to_insert)\n",
    "        \n",
    "        print(f\"  Inserted {len(rows_to_insert)} rows into '{output_small_hyper_file}'.\")\n",
    "\n",
    "finally:\n",
    "    # Ensure connection and hyper process are closed, even if errors occur\n",
    "    if connection:\n",
    "        connection.close()\n",
    "        print(\"Hyper connection closed.\")\n",
    "    if hyper_process:\n",
    "        hyper_process.close()\n",
    "        print(\"Hyper process closed.\")\n",
    "\n",
    "print(\"\\nSmall test export process finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8c7ab94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 34 parquet files to export to CSV.\n",
      "Processing each parquet file individually, enforcing schema and order...\n",
      "  Reading and processing batch 1/34: batch_001.parquet\n",
      "  Reading and processing batch 2/34: batch_002.parquet\n",
      "  Reading and processing batch 3/34: batch_003.parquet\n",
      "  Reading and processing batch 4/34: batch_004.parquet\n",
      "  Reading and processing batch 5/34: batch_005.parquet\n",
      "  Reading and processing batch 6/34: batch_006.parquet\n",
      "  Reading and processing batch 7/34: batch_007.parquet\n",
      "  Reading and processing batch 8/34: batch_008.parquet\n",
      "  Reading and processing batch 9/34: batch_009.parquet\n",
      "  Reading and processing batch 10/34: batch_010.parquet\n",
      "  Reading and processing batch 11/34: batch_011.parquet\n",
      "  Reading and processing batch 12/34: batch_012.parquet\n",
      "  Reading and processing batch 13/34: batch_013.parquet\n",
      "  Reading and processing batch 14/34: batch_014.parquet\n",
      "  Reading and processing batch 15/34: batch_015.parquet\n",
      "  Reading and processing batch 16/34: batch_016.parquet\n",
      "  Reading and processing batch 17/34: batch_017.parquet\n",
      "  Reading and processing batch 18/34: batch_018.parquet\n",
      "  Reading and processing batch 19/34: batch_019.parquet\n",
      "  Reading and processing batch 20/34: batch_020.parquet\n",
      "  Reading and processing batch 21/34: batch_021.parquet\n",
      "  Reading and processing batch 22/34: batch_022.parquet\n",
      "  Reading and processing batch 23/34: batch_023.parquet\n",
      "  Reading and processing batch 24/34: batch_024.parquet\n",
      "  Reading and processing batch 25/34: batch_025.parquet\n",
      "  Reading and processing batch 26/34: batch_026.parquet\n",
      "  Reading and processing batch 27/34: batch_027.parquet\n",
      "  Reading and processing batch 28/34: batch_028.parquet\n",
      "  Reading and processing batch 29/34: batch_029.parquet\n",
      "  Reading and processing batch 30/34: batch_030.parquet\n",
      "  Reading and processing batch 31/34: batch_031.parquet\n",
      "  Reading and processing batch 32/34: batch_032.parquet\n",
      "  Reading and processing batch 33/34: batch_033.parquet\n",
      "  Reading and processing batch 34/34: batch_034.parquet\n",
      "Concatenating all processed DataFrames...\n",
      "Combined DataFrame loaded successfully with 94869699 rows.\n",
      "Exporting data to CSV: /Users/tusharjoshi/Desktop/ProjectWorkAll/Dissertation /printer_data_full.csv...\n",
      "Successfully exported all data to '/Users/tusharjoshi/Desktop/ProjectWorkAll/Dissertation /printer_data_full.csv'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import polars as pl\n",
    "\n",
    "# --- Configuration ---\n",
    "# IMPORTANT: Update this path to your actual folder containing Parquet files.\n",
    "parquet_folder = \"/Users/tusharjoshi/Desktop/ProjectWorkAll/Dissertation /processed_float\" \n",
    "\n",
    "# This is where your new CSV file will be created.\n",
    "output_csv_file = \"/Users/tusharjoshi/Desktop/ProjectWorkAll/Dissertation /printer_data_full.csv\" \n",
    "\n",
    "# --- Define the Target Schema and Final Column Order ---\n",
    "# This schema defines the data type for each column.\n",
    "target_schema = {\n",
    "    \"check\": pl.String,\n",
    "    \"date\": pl.String, # Read as string, then parse to datetime\n",
    "    \"id\": pl.String,    # Ensure ID can be null or string\n",
    "    \"state\": pl.String, # Ensure State can be null or string\n",
    "    \"tempBed\": pl.Float64,\n",
    "    \"targetBed\": pl.Float64,\n",
    "    \"tempNozzle\": pl.Float64,\n",
    "    \"targetNozzle\": pl.Float64,\n",
    "    \"axisZ\": pl.Float64,\n",
    "    \"axisX\": pl.Float64,\n",
    "    \"axisY\": pl.Float64,\n",
    "    \"flow\": pl.Float64,\n",
    "    \"speed\": pl.Float64,\n",
    "    \"fanHotend\": pl.Float64,\n",
    "    \"fanPrint\": pl.Float64,\n",
    "}\n",
    "\n",
    "# This defines the exact order of columns for the final DataFrame.\n",
    "# It's crucial that all columns in target_schema are also listed here.\n",
    "final_column_order = [\n",
    "    \"check\", \"date\", \"id\", \"state\", \"tempBed\", \"targetBed\", \"tempNozzle\", \n",
    "    \"targetNozzle\", \"axisZ\", \"axisX\", \"axisY\", \"flow\", \"speed\", \n",
    "    \"fanHotend\", \"fanPrint\"\n",
    "]\n",
    "\n",
    "# --- Main Export Process ---\n",
    "try:\n",
    "    # List all parquet files in the folder and sort them\n",
    "    all_parquet_files = sorted([\n",
    "        os.path.join(parquet_folder, f)\n",
    "        for f in os.listdir(parquet_folder)\n",
    "        if f.endswith(\".parquet\")\n",
    "    ])\n",
    "    print(f\"Found {len(all_parquet_files)} parquet files to export to CSV.\")\n",
    "\n",
    "    if not all_parquet_files:\n",
    "        print(\"No parquet files found in the specified folder. Nothing to export.\")\n",
    "    else:\n",
    "        processed_dfs = []\n",
    "        print(\"Processing each parquet file individually, enforcing schema and order...\")\n",
    "        \n",
    "        for i, parquet_file in enumerate(all_parquet_files):\n",
    "            print(f\"  Reading and processing batch {i+1}/{len(all_parquet_files)}: {os.path.basename(parquet_file)}\")\n",
    "            \n",
    "            # Read the parquet file\n",
    "            df_batch = pl.read_parquet(parquet_file)\n",
    "            \n",
    "            # Ensure all columns from final_column_order are present and in the correct type\n",
    "            # Add missing columns with nulls, and cast existing ones\n",
    "            for col_name in final_column_order:\n",
    "                if col_name not in df_batch.columns:\n",
    "                    # Add missing column with nulls and correct type\n",
    "                    df_batch = df_batch.with_columns(pl.lit(None, dtype=target_schema[col_name]).alias(col_name))\n",
    "                else:\n",
    "                    # Cast existing column to the target type\n",
    "                    df_batch = df_batch.with_columns(pl.col(col_name).cast(target_schema[col_name]))\n",
    "            \n",
    "            # Select and reorder columns to match final_column_order\n",
    "            df_batch = df_batch.select(final_column_order)\n",
    "            \n",
    "            processed_dfs.append(df_batch)\n",
    "        \n",
    "        # Concatenate all processed DataFrames into one\n",
    "        print(\"Concatenating all processed DataFrames...\")\n",
    "        df_combined = pl.concat(processed_dfs)\n",
    "        print(f\"Combined DataFrame loaded successfully with {df_combined.shape[0]} rows.\")\n",
    "\n",
    "        # Re-parse the 'date' column to actual datetime objects, handling errors by setting to null\n",
    "        df_combined = df_combined.with_columns(\n",
    "            pl.col(\"date\").str.strptime(pl.Datetime, format=\"%Y-%m-%dT%H:%M:%S%.f%Z\", strict=False).alias(\"date\")\n",
    "        )\n",
    "\n",
    "        # Export the DataFrame to a single CSV file\n",
    "        print(f\"Exporting data to CSV: {output_csv_file}...\")\n",
    "        df_combined.write_csv(output_csv_file)\n",
    "        print(f\"Successfully exported all data to '{output_csv_file}'.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn error occurred during the CSV export process: {e}\")\n",
    "    print(\"Please check:\")\n",
    "    print(f\"- The 'parquet_folder' path: {parquet_folder}\")\n",
    "    print(f\"- The 'output_csv_file' path: {output_csv_file}\")\n",
    "    print(\"- That you have enough disk space for the CSV file.\")\n",
    "    print(\"- That the column names in your Parquet files are consistent enough to be mapped to the 'target_schema'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Printer Env)",
   "language": "python",
   "name": "printer-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
