{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ca5fbb0",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "'cryptography' package is required for sha256_password or caching_sha2_password auth methods",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtime\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Connect\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m connection = \u001b[43mpymysql\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhost\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlocalhost\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43muser\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mroot\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtusharadmin15\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdatabase\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mprinter_data_db\u001b[39;49m\u001b[33;43m'\u001b[39;49m\n\u001b[32m     12\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m60\u001b[39m)\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mFAST 1M SAMPLE LOADING\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/pymysql/connections.py:361\u001b[39m, in \u001b[36mConnection.__init__\u001b[39m\u001b[34m(self, user, password, host, database, unix_socket, port, charset, collation, sql_mode, read_default_file, conv, use_unicode, client_flag, cursorclass, init_command, connect_timeout, read_default_group, autocommit, local_infile, max_allowed_packet, defer_connect, auth_plugin_map, read_timeout, write_timeout, bind_address, binary_prefix, program_name, server_public_key, ssl, ssl_ca, ssl_cert, ssl_disabled, ssl_key, ssl_key_password, ssl_verify_cert, ssl_verify_identity, compress, named_pipe, passwd, db)\u001b[39m\n\u001b[32m    359\u001b[39m     \u001b[38;5;28mself\u001b[39m._sock = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    360\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m361\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/pymysql/connections.py:669\u001b[39m, in \u001b[36mConnection.connect\u001b[39m\u001b[34m(self, sock)\u001b[39m\n\u001b[32m    666\u001b[39m \u001b[38;5;28mself\u001b[39m._next_seq_id = \u001b[32m0\u001b[39m\n\u001b[32m    668\u001b[39m \u001b[38;5;28mself\u001b[39m._get_server_information()\n\u001b[32m--> \u001b[39m\u001b[32m669\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request_authentication\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    671\u001b[39m \u001b[38;5;66;03m# Send \"SET NAMES\" query on init for:\u001b[39;00m\n\u001b[32m    672\u001b[39m \u001b[38;5;66;03m# - Ensure charaset (and collation) is set to the server.\u001b[39;00m\n\u001b[32m    673\u001b[39m \u001b[38;5;66;03m#   - collation_id in handshake packet may be ignored.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    682\u001b[39m \u001b[38;5;66;03m# - https://github.com/wagtail/wagtail/issues/9477\u001b[39;00m\n\u001b[32m    683\u001b[39m \u001b[38;5;66;03m# - https://zenn.dev/methane/articles/2023-mysql-collation (Japanese)\u001b[39;00m\n\u001b[32m    684\u001b[39m \u001b[38;5;28mself\u001b[39m.set_character_set(\u001b[38;5;28mself\u001b[39m.charset, \u001b[38;5;28mself\u001b[39m.collation)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/pymysql/connections.py:979\u001b[39m, in \u001b[36mConnection._request_authentication\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    977\u001b[39m \u001b[38;5;66;03m# https://dev.mysql.com/doc/internals/en/successful-authentication.html\u001b[39;00m\n\u001b[32m    978\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._auth_plugin_name == \u001b[33m\"\u001b[39m\u001b[33mcaching_sha2_password\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m979\u001b[39m     auth_packet = \u001b[43m_auth\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcaching_sha2_password_auth\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauth_packet\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._auth_plugin_name == \u001b[33m\"\u001b[39m\u001b[33msha256_password\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    981\u001b[39m     auth_packet = _auth.sha256_password_auth(\u001b[38;5;28mself\u001b[39m, auth_packet)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/pymysql/_auth.py:267\u001b[39m, in \u001b[36mcaching_sha2_password_auth\u001b[39m\u001b[34m(conn, pkt)\u001b[39m\n\u001b[32m    264\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m DEBUG:\n\u001b[32m    265\u001b[39m         \u001b[38;5;28mprint\u001b[39m(conn.server_public_key.decode(\u001b[33m\"\u001b[39m\u001b[33mascii\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m--> \u001b[39m\u001b[32m267\u001b[39m data = \u001b[43msha2_rsa_encrypt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpassword\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43msalt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mserver_public_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    268\u001b[39m pkt = _roundtrip(conn, data)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/pymysql/_auth.py:144\u001b[39m, in \u001b[36msha2_rsa_encrypt\u001b[39m\u001b[34m(password, salt, public_key)\u001b[39m\n\u001b[32m    139\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Encrypt password with salt and public_key.\u001b[39;00m\n\u001b[32m    140\u001b[39m \n\u001b[32m    141\u001b[39m \u001b[33;03mUsed for sha256_password and caching_sha2_password.\u001b[39;00m\n\u001b[32m    142\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    143\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _have_cryptography:\n\u001b[32m--> \u001b[39m\u001b[32m144\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    145\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[33mcryptography\u001b[39m\u001b[33m'\u001b[39m\u001b[33m package is required for sha256_password or\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    146\u001b[39m         + \u001b[33m\"\u001b[39m\u001b[33m caching_sha2_password auth methods\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    147\u001b[39m     )\n\u001b[32m    148\u001b[39m message = _xor_password(password + \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\0\u001b[39;00m\u001b[33m\"\u001b[39m, salt)\n\u001b[32m    149\u001b[39m rsa_key = serialization.load_pem_public_key(public_key, default_backend())\n",
      "\u001b[31mRuntimeError\u001b[39m: 'cryptography' package is required for sha256_password or caching_sha2_password auth methods"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pymysql\n",
    "import time\n",
    "\n",
    "# Connect\n",
    "connection = pymysql.connect(\n",
    "    host='localhost',\n",
    "    user='root',\n",
    "    password='tusharadmin15',\n",
    "    database='printer_data_db'\n",
    ")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"FAST 1M SAMPLE LOADING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Step 1: Get printer list and their record counts\n",
    "print(\"\\nStep 1: Getting printer distribution...\")\n",
    "printer_query = \"\"\"\n",
    "SELECT \n",
    "    `id` as printer_id,\n",
    "    COUNT(*) as count\n",
    "FROM PrinterData\n",
    "GROUP BY `id`\n",
    "ORDER BY count DESC\n",
    "\"\"\"\n",
    "printer_dist = pd.read_sql(printer_query, connection)\n",
    "print(f\"Found {len(printer_dist)} printers\")\n",
    "\n",
    "# Step 2: Calculate samples needed per printer\n",
    "total_sample = 1000000\n",
    "printer_dist['samples_needed'] = (\n",
    "    printer_dist['count'] / printer_dist['count'].sum() * total_sample\n",
    ").astype(int)\n",
    "\n",
    "# Ensure minimum 50k per printer for good clustering\n",
    "printer_dist['samples_needed'] = printer_dist['samples_needed'].clip(lower=50000)\n",
    "\n",
    "# Adjust to maintain 1M total\n",
    "scale_factor = total_sample / printer_dist['samples_needed'].sum()\n",
    "printer_dist['samples_needed'] = (printer_dist['samples_needed'] * scale_factor).astype(int)\n",
    "\n",
    "print(\"\\nSamples per printer:\")\n",
    "print(printer_dist[['printer_id', 'samples_needed']].head(8))\n",
    "\n",
    "# Step 3: Load samples using LIMIT with random ordering\n",
    "df_list = []\n",
    "for idx, row in printer_dist.iterrows():\n",
    "    printer_id = row['printer_id']\n",
    "    samples = min(row['samples_needed'], row['count'])  # Don't exceed available\n",
    "    \n",
    "    print(f\"\\nLoading {samples:,} from {printer_id[-6:]}...\", end='')\n",
    "    \n",
    "    # Simple query with LIMIT - much faster than window functions\n",
    "    query = f\"\"\"\n",
    "    SELECT \n",
    "        `id`, `date`, `state`,\n",
    "        tempBed, targetBed, tempNozzle, targetNozzle,\n",
    "        flow, speed, fanHotend, fanPrint\n",
    "    FROM PrinterData\n",
    "    WHERE `id` = '{printer_id}'\n",
    "    LIMIT {samples}\n",
    "    \"\"\"\n",
    "    \n",
    "    df_temp = pd.read_sql(query, connection)\n",
    "    df_list.append(df_temp)\n",
    "    print(f\" Done! ({len(df_temp):,} records)\")\n",
    "\n",
    "# Combine all samples\n",
    "print(\"\\nCombining samples...\")\n",
    "df_sample = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# Shuffle to mix printers\n",
    "print(\"Shuffling data...\")\n",
    "df_sample = df_sample.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SAMPLING COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total records: {len(df_sample):,}\")\n",
    "print(f\"Time taken: {end_time - start_time:.1f} seconds\")\n",
    "print(f\"\\nPrinter distribution in sample:\")\n",
    "print(df_sample['id'].value_counts())\n",
    "print(f\"\\nState distribution in sample:\")\n",
    "print(df_sample['state'].value_counts())\n",
    "\n",
    "# Save sample\n",
    "df_sample.to_pickle('sample_1M_balanced.pkl')\n",
    "print(\"\\nSample saved to sample_1M_balanced.pkl\")\n",
    "\n",
    "# Close connection\n",
    "connection.close()\n",
    "print(\"Database connection closed\")\n",
    "\n",
    "#feature engineering \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"PHASE 2: FEATURE ENGINEERING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load the sample we just created\n",
    "print(\"\\nLoading saved sample...\")\n",
    "df_sample = pd.read_pickle('sample_1M_balanced.pkl')\n",
    "print(f\"Loaded {len(df_sample):,} records\")\n",
    "\n",
    "# Create the 7 features we've been using\n",
    "print(\"\\nCreating features...\")\n",
    "df_sample['bed_error'] = abs(df_sample['tempBed'] - df_sample['targetBed'])\n",
    "df_sample['nozzle_error'] = abs(df_sample['tempNozzle'] - df_sample['targetNozzle'])\n",
    "df_sample['total_temp_error'] = df_sample['bed_error'] + df_sample['nozzle_error']\n",
    "df_sample['efficiency'] = (df_sample['speed'] * df_sample['flow']) / 10000\n",
    "df_sample['fan_ratio'] = df_sample['fanHotend'] / (df_sample['fanPrint'] + 1)\n",
    "\n",
    "# Keep original features\n",
    "df_sample['flow_feature'] = df_sample['flow']\n",
    "df_sample['speed_feature'] = df_sample['speed']\n",
    "\n",
    "feature_columns = ['bed_error', 'nozzle_error', 'total_temp_error', \n",
    "                   'efficiency', 'fan_ratio', 'flow_feature', 'speed_feature']\n",
    "\n",
    "print(\"\\nFeature statistics:\")\n",
    "print(df_sample[feature_columns].describe())\n",
    "\n",
    "# Check for any NaN values\n",
    "nan_counts = df_sample[feature_columns].isna().sum()\n",
    "print(\"\\nNaN values per feature:\")\n",
    "print(nan_counts)\n",
    "# Handle any NaN values\n",
    "print(\"\\nHandling missing values...\")\n",
    "df_sample[feature_columns] = df_sample[feature_columns].fillna(0)\n",
    "\n",
    "# Check for extreme outliers\n",
    "print(\"\\nChecking for extreme outliers...\")\n",
    "for col in feature_columns:\n",
    "    q99 = df_sample[col].quantile(0.99)\n",
    "    q999 = df_sample[col].quantile(0.999)\n",
    "    max_val = df_sample[col].max()\n",
    "    print(f\"{col:20} - 99%: {q99:.2f}, 99.9%: {q999:.2f}, Max: {max_val:.2f}\")\n",
    "\n",
    "# Cap extreme outliers at 99.9th percentile\n",
    "print(\"\\nCapping extreme outliers...\")\n",
    "for col in feature_columns:\n",
    "    cap_value = df_sample[col].quantile(0.999)\n",
    "    df_sample[col] = df_sample[col].clip(upper=cap_value)\n",
    "    \n",
    "print(\"Outliers capped at 99.9th percentile\")\n",
    "#feature scaling \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "\n",
    "print(\"\\nScaling features...\")\n",
    "scaler = StandardScaler()\n",
    "X = df_sample[feature_columns].values\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(f\"Scaled data shape: {X_scaled.shape}\")\n",
    "print(f\"Scaled data mean: {X_scaled.mean():.4f} (should be ~0)\")\n",
    "print(f\"Scaled data std: {X_scaled.std():.4f} (should be ~1)\")\n",
    "\n",
    "# Save scaler for later use\n",
    "with open('scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "print(\"\\nScaler saved to scaler.pkl\")\n",
    "\n",
    "# Add scaled features back to dataframe for analysis\n",
    "for i, col in enumerate(feature_columns):\n",
    "    df_sample[f'{col}_scaled'] = X_scaled[:, i]\n",
    "\n",
    "#let us visualise these feature distribution\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "\n",
    "# Analyze how features differ by state\n",
    "print(\"\\nFeature means by state:\")\n",
    "state_features = df_sample.groupby('state')[feature_columns].mean()\n",
    "print(state_features)\n",
    "\n",
    "# Visualize key differences\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "fig.suptitle('Feature Distributions by State', fontsize=16)\n",
    "\n",
    "key_features = ['bed_error', 'nozzle_error', 'flow_feature', \n",
    "                'speed_feature', 'efficiency', 'fan_ratio']\n",
    "\n",
    "for idx, feature in enumerate(key_features):\n",
    "    ax = axes[idx // 3, idx % 3]\n",
    "    \n",
    "    # Box plot by state\n",
    "    df_sample.boxplot(column=feature, by='state', ax=ax)\n",
    "    ax.set_title(feature)\n",
    "    ax.set_xlabel('State')\n",
    "    ax.set_ylabel('Value')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nObservations:\")\n",
    "print(\"1. IDLE state has high temperature errors (cold printer)\")\n",
    "print(\"2. PRINTING state has low errors, high flow/speed\")\n",
    "print(\"3. FINISHED state shows cooling patterns\")\n",
    "print(\"4. This confirms different states need different treatment\")\n",
    "\n",
    "# Dimensionality Reduction \n",
    "#PCA first look\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"PHASE 3: DIMENSIONALITY REDUCTION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Try PCA first to understand variance\n",
    "print(\"\\nApplying PCA for initial analysis...\")\n",
    "pca_test = PCA()\n",
    "pca_test.fit(X_scaled)\n",
    "\n",
    "# Explained variance\n",
    "explained_var = pca_test.explained_variance_ratio_\n",
    "cumsum_var = np.cumsum(explained_var)\n",
    "\n",
    "print(\"\\nPCA Explained Variance:\")\n",
    "for i, (var, cum) in enumerate(zip(explained_var, cumsum_var)):\n",
    "    print(f\"PC{i+1}: {var*100:.2f}% (Cumulative: {cum*100:.2f}%)\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(range(1, len(explained_var)+1), explained_var)\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.title('PCA Explained Variance')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, len(cumsum_var)+1), cumsum_var, 'bo-')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('Cumulative Variance Explained')\n",
    "plt.axhline(y=0.95, color='r', linestyle='--', label='95% threshold')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nComponents needed for 95% variance: {np.argmax(cumsum_var >= 0.95) + 1}\")\n",
    "\n",
    "#Applying UMAP for clustering \n",
    "import umap\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "print(\"\\nApplying UMAP for better cluster separation...\")\n",
    "print(\"This will take 5-10 minutes for 1M records...\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Configure UMAP\n",
    "reducer = umap.UMAP(\n",
    "    n_components=3,        # 3D for visualization\n",
    "    n_neighbors=30,        # Smaller than before for faster processing\n",
    "    min_dist=0.1,         \n",
    "    metric='euclidean',\n",
    "    random_state=42,\n",
    "    verbose=True,\n",
    "    n_jobs=-1             # Use all CPU cores\n",
    ")\n",
    "\n",
    "# Fit and transform\n",
    "X_reduced = reducer.fit_transform(X_scaled)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"\\nUMAP completed in {(end_time - start_time)/60:.1f} minutes\")\n",
    "\n",
    "# Save reducer\n",
    "with open('umap_reducer.pkl', 'wb') as f:\n",
    "    pickle.dump(reducer, f)\n",
    "print(\"UMAP reducer saved\")\n",
    "\n",
    "# Add to dataframe\n",
    "df_sample['UMAP1'] = X_reduced[:, 0]\n",
    "df_sample['UMAP2'] = X_reduced[:, 1]  \n",
    "df_sample['UMAP3'] = X_reduced[:, 2]\n",
    "\n",
    "print(f\"\\nReduced dimensions shape: {X_reduced.shape}\")\n",
    "print(\"UMAP components added to dataframe\")\n",
    "\n",
    "# Quick 2D visualization (faster than 3D)\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Plot 1: Colored by state\n",
    "plt.subplot(1, 3, 1)\n",
    "states = df_sample['state'].unique()\n",
    "colors = plt.cm.tab10(np.linspace(0, 1, len(states)))\n",
    "\n",
    "for state, color in zip(states, colors):\n",
    "    mask = df_sample['state'] == state\n",
    "    plt.scatter(X_reduced[mask, 0], X_reduced[mask, 1], \n",
    "               c=[color], label=state, alpha=0.5, s=1)\n",
    "\n",
    "plt.xlabel('UMAP1')\n",
    "plt.ylabel('UMAP2')\n",
    "plt.title('UMAP Projection by State')\n",
    "plt.legend()\n",
    "\n",
    "# Plot 2: Colored by printer\n",
    "plt.subplot(1, 3, 2)\n",
    "sample_printers = df_sample['id'].unique()\n",
    "colors = plt.cm.tab10(np.linspace(0, 1, len(sample_printers)))\n",
    "\n",
    "for printer, color in zip(sample_printers[:4], colors[:4]):  # Show first 4 printers\n",
    "    mask = df_sample['id'] == printer\n",
    "    plt.scatter(X_reduced[mask, 0], X_reduced[mask, 1], \n",
    "               c=[color], label=printer[-6:], alpha=0.5, s=1)\n",
    "\n",
    "plt.xlabel('UMAP1')\n",
    "plt.ylabel('UMAP2')\n",
    "plt.title('UMAP Projection by Printer (First 4)')\n",
    "plt.legend()\n",
    "\n",
    "# Plot 3: Colored by temperature error\n",
    "plt.subplot(1, 3, 3)\n",
    "scatter = plt.scatter(X_reduced[:, 0], X_reduced[:, 1], \n",
    "                     c=df_sample['total_temp_error'], \n",
    "                     cmap='coolwarm', s=1, alpha=0.5)\n",
    "plt.colorbar(scatter)\n",
    "plt.xlabel('UMAP1')\n",
    "plt.ylabel('UMAP2')\n",
    "plt.title('UMAP Colored by Total Temp Error')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nInitial observations from UMAP:\")\n",
    "print(\"- Look for distinct clusters\")\n",
    "print(\"- Check if states separate naturally\")\n",
    "print(\"- Identify potential anomaly regions\")\n",
    "\n",
    "#DBSCAN Clustering\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import cdist\n",
    "import pickle\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"SIMPLIFIED ANOMALY DETECTION (REPLACING PHASE 4 & 5)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Make sure we have the UMAP data\n",
    "print(\"\\nUsing UMAP reduced data for anomaly detection...\")\n",
    "X_umap = df_sample[['UMAP1', 'UMAP2', 'UMAP3']].values\n",
    "\n",
    "# Step 1: Calculate distance from center of main distribution\n",
    "print(\"\\nStep 1: Calculating distances from distribution center...\")\n",
    "\n",
    "# Find the center of the main cluster (where most normal data is)\n",
    "center = np.median(X_umap, axis=0)  # Use median to be robust to outliers\n",
    "print(f\"Distribution center: [{center[0]:.2f}, {center[1]:.2f}, {center[2]:.2f}]\")\n",
    "\n",
    "# Calculate distance of each point from center\n",
    "distances = np.sqrt(((X_umap - center) ** 2).sum(axis=1))\n",
    "df_sample['distance_from_center'] = distances\n",
    "\n",
    "print(f\"Distance statistics:\")\n",
    "print(f\"  Min: {distances.min():.2f}\")\n",
    "print(f\"  Median: {np.median(distances):.2f}\")\n",
    "print(f\"  Max: {distances.max():.2f}\")\n",
    "\n",
    "# Step 2: Label anomalies based on multiple criteria\n",
    "print(\"\\nStep 2: Labeling anomalies using multiple criteria...\")\n",
    "\n",
    "# Initialize all as normal\n",
    "df_sample['is_anomaly'] = 0\n",
    "anomaly_reasons = []\n",
    "\n",
    "# Criterion 1: Distance-based (top 2% furthest points)\n",
    "distance_threshold = np.percentile(distances, 98)\n",
    "distance_anomalies = distances > distance_threshold\n",
    "df_sample.loc[distance_anomalies, 'is_anomaly'] = 1\n",
    "print(f\"  Distance-based anomalies (top 2%): {distance_anomalies.sum():,}\")\n",
    "\n",
    "# Criterion 2: STOPPED state (always anomaly)\n",
    "stopped_mask = df_sample['state'] == 'STOPPED'\n",
    "df_sample.loc[stopped_mask, 'is_anomaly'] = 1\n",
    "print(f\"  STOPPED state anomalies: {stopped_mask.sum():,}\")\n",
    "\n",
    "# Criterion 3: Extreme temperature errors\n",
    "extreme_temp = (df_sample['bed_error'] > 40) | (df_sample['nozzle_error'] > 80)\n",
    "df_sample.loc[extreme_temp, 'is_anomaly'] = 1\n",
    "print(f\"  Extreme temperature anomalies: {extreme_temp.sum():,}\")\n",
    "\n",
    "# Criterion 4: PRINTING with no flow (shouldn't happen)\n",
    "printing_no_flow = (df_sample['state'] == 'PRINTING') & (df_sample['flow_feature'] < 10)\n",
    "df_sample.loc[printing_no_flow, 'is_anomaly'] = 1\n",
    "print(f\"  Printing without flow anomalies: {printing_no_flow.sum():,}\")\n",
    "\n",
    "# Criterion 5: Hot while IDLE (shouldn't happen)\n",
    "idle_hot = (df_sample['state'] == 'IDLE') & ((df_sample['tempBed'] > 30) | (df_sample['tempNozzle'] > 50))\n",
    "df_sample.loc[idle_hot, 'is_anomaly'] = 1\n",
    "print(f\"  Hot while idle anomalies: {idle_hot.sum():,}\")\n",
    "\n",
    "# Calculate final statistics\n",
    "total_anomalies = df_sample['is_anomaly'].sum()\n",
    "anomaly_rate = total_anomalies / len(df_sample) * 100\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ANOMALY LABELING COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total records: {len(df_sample):,}\")\n",
    "print(f\"Total anomalies: {total_anomalies:,}\")\n",
    "print(f\"Anomaly rate: {anomaly_rate:.2f}%\")\n",
    "print(f\"Normal records: {len(df_sample) - total_anomalies:,}\")\n",
    "\n",
    "# Step 3: Validate the labels make sense\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"VALIDATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check anomaly rate by state\n",
    "print(\"\\nAnomaly rate by state:\")\n",
    "for state in ['IDLE', 'FINISHED', 'PRINTING', 'BUSY', 'STOPPED']:\n",
    "    state_data = df_sample[df_sample['state'] == state]\n",
    "    if len(state_data) > 0:\n",
    "        state_anomalies = state_data['is_anomaly'].sum()\n",
    "        state_rate = state_anomalies / len(state_data) * 100\n",
    "        print(f\"  {state:10} {state_anomalies:6,} / {len(state_data):6,} = {state_rate:5.1f}%\")\n",
    "\n",
    "# Compare features of normal vs anomaly\n",
    "print(\"\\nFeature comparison (Normal vs Anomaly):\")\n",
    "feature_cols = ['bed_error', 'nozzle_error', 'flow_feature', 'speed_feature']\n",
    "normal_means = df_sample[df_sample['is_anomaly'] == 0][feature_cols].mean()\n",
    "anomaly_means = df_sample[df_sample['is_anomaly'] == 1][feature_cols].mean()\n",
    "\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Normal': normal_means,\n",
    "    'Anomaly': anomaly_means,\n",
    "    'Difference': anomaly_means - normal_means\n",
    "})\n",
    "print(comparison_df.round(2))\n",
    "\n",
    "# Step 4: Visualize the results\n",
    "print(\"\\nCreating visualization...\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Plot 1: Anomalies in UMAP space\n",
    "ax1 = axes[0, 0]\n",
    "normal_mask = df_sample['is_anomaly'] == 0\n",
    "anomaly_mask = df_sample['is_anomaly'] == 1\n",
    "\n",
    "ax1.scatter(df_sample[normal_mask]['UMAP1'], \n",
    "           df_sample[normal_mask]['UMAP2'],\n",
    "           c='blue', s=1, alpha=0.5, label='Normal')\n",
    "ax1.scatter(df_sample[anomaly_mask]['UMAP1'], \n",
    "           df_sample[anomaly_mask]['UMAP2'],\n",
    "           c='red', s=5, alpha=0.8, label='Anomaly')\n",
    "ax1.set_xlabel('UMAP1')\n",
    "ax1.set_ylabel('UMAP2')\n",
    "ax1.set_title(f'Anomalies in UMAP Space ({anomaly_rate:.1f}% anomalies)')\n",
    "ax1.legend()\n",
    "\n",
    "# Plot 2: Distance distribution\n",
    "ax2 = axes[0, 1]\n",
    "ax2.hist(distances, bins=100, alpha=0.7, color='blue', label='All points')\n",
    "ax2.axvline(distance_threshold, color='red', linestyle='--', \n",
    "           label=f'Threshold (98th percentile)')\n",
    "ax2.set_xlabel('Distance from Center')\n",
    "ax2.set_ylabel('Count')\n",
    "ax2.set_title('Distance Distribution')\n",
    "ax2.legend()\n",
    "\n",
    "# Plot 3: Anomaly rate by printer\n",
    "ax3 = axes[1, 0]\n",
    "printer_anomaly_rates = []\n",
    "printer_names = []\n",
    "for printer in df_sample['id'].unique():\n",
    "    printer_data = df_sample[df_sample['id'] == printer]\n",
    "    rate = printer_data['is_anomaly'].mean() * 100\n",
    "    printer_anomaly_rates.append(rate)\n",
    "    printer_names.append(printer[-6:])  # Last 6 chars\n",
    "\n",
    "ax3.bar(range(len(printer_names)), printer_anomaly_rates)\n",
    "ax3.set_xticks(range(len(printer_names)))\n",
    "ax3.set_xticklabels(printer_names, rotation=45)\n",
    "ax3.set_ylabel('Anomaly Rate (%)')\n",
    "ax3.set_title('Anomaly Rate by Printer')\n",
    "ax3.axhline(y=anomaly_rate, color='r', linestyle='--', alpha=0.5, label='Overall rate')\n",
    "ax3.legend()\n",
    "\n",
    "# Plot 4: Feature comparison\n",
    "ax4 = axes[1, 1]\n",
    "x = np.arange(len(feature_cols))\n",
    "width = 0.35\n",
    "\n",
    "normal_vals = [normal_means[f] for f in feature_cols]\n",
    "anomaly_vals = [anomaly_means[f] for f in feature_cols]\n",
    "\n",
    "ax4.bar(x - width/2, normal_vals, width, label='Normal', color='blue', alpha=0.7)\n",
    "ax4.bar(x + width/2, anomaly_vals, width, label='Anomaly', color='red', alpha=0.7)\n",
    "ax4.set_xlabel('Features')\n",
    "ax4.set_ylabel('Average Value')\n",
    "ax4.set_title('Feature Comparison: Normal vs Anomaly')\n",
    "ax4.set_xticks(x)\n",
    "ax4.set_xticklabels(feature_cols, rotation=45)\n",
    "ax4.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Step 5: Save the labeled data\n",
    "print(\"\\nSaving labeled dataset...\")\n",
    "\n",
    "# Save full dataset\n",
    "df_sample.to_pickle('labeled_data_final.pkl')\n",
    "\n",
    "# Save features and labels for modeling\n",
    "X_features = df_sample[feature_columns].values\n",
    "y_labels = df_sample['is_anomaly'].values\n",
    "\n",
    "np.save('X_features_final.npy', X_features)\n",
    "np.save('y_labels_final.npy', y_labels)\n",
    "\n",
    "print(\"âœ“ Dataset saved to labeled_data_final.pkl\")\n",
    "print(\"âœ“ Features saved to X_features_final.npy\")\n",
    "print(\"âœ“ Labels saved to y_labels_final.npy\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"READY FOR PHASE 6: SUPERVISED MODEL TRAINING\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Summary:\")\n",
    "print(f\"  â€¢ Total records: {len(df_sample):,}\")\n",
    "print(f\"  â€¢ Anomalies: {total_anomalies:,} ({anomaly_rate:.2f}%)\")\n",
    "print(f\"  â€¢ Features: {len(feature_columns)}\")\n",
    "print(f\"  â€¢ Next step: Train supervised model on these labels\")\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"3D INTERACTIVE VISUALIZATION OF ANOMALIES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create interactive 3D scatter plot\n",
    "fig = px.scatter_3d(\n",
    "    df_sample.sample(min(50000, len(df_sample))),  # Sample for performance\n",
    "    x='UMAP1', \n",
    "    y='UMAP2', \n",
    "    z='UMAP3',\n",
    "    color='is_anomaly',\n",
    "    color_discrete_map={0: 'blue', 1: 'red'},\n",
    "    labels={'is_anomaly': 'Anomaly Status'},\n",
    "    title=f'3D Anomaly Detection Results ({anomaly_rate:.1f}% anomalies)',\n",
    "    hover_data=['state', 'bed_error', 'nozzle_error', 'id']\n",
    ")\n",
    "\n",
    "# Update marker properties\n",
    "fig.update_traces(\n",
    "    marker=dict(\n",
    "        size=3,\n",
    "        opacity=0.7,\n",
    "        line=dict(width=0)\n",
    "    )\n",
    ")\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    scene=dict(\n",
    "        xaxis_title='UMAP Dimension 1',\n",
    "        yaxis_title='UMAP Dimension 2',\n",
    "        zaxis_title='UMAP Dimension 3',\n",
    "        bgcolor='white'\n",
    "    ),\n",
    "    height=700,\n",
    "    showlegend=True\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Second visualization: Colored by state with anomalies highlighted\n",
    "print(\"\\nCreating state-based 3D visualization...\")\n",
    "\n",
    "fig2 = go.Figure()\n",
    "\n",
    "# Add normal points colored by state\n",
    "states = df_sample['state'].unique()\n",
    "colors = px.colors.qualitative.Plotly\n",
    "\n",
    "for i, state in enumerate(states):\n",
    "    if state and state != '':  # Skip empty states\n",
    "        state_mask = (df_sample['state'] == state) & (df_sample['is_anomaly'] == 0)\n",
    "        state_data = df_sample[state_mask].sample(min(5000, state_mask.sum()))\n",
    "        \n",
    "        fig2.add_trace(go.Scatter3d(\n",
    "            x=state_data['UMAP1'],\n",
    "            y=state_data['UMAP2'],\n",
    "            z=state_data['UMAP3'],\n",
    "            mode='markers',\n",
    "            name=f'{state} (Normal)',\n",
    "            marker=dict(\n",
    "                size=2,\n",
    "                color=colors[i % len(colors)],\n",
    "                opacity=0.6\n",
    "            ),\n",
    "            text=state_data['state'],\n",
    "            hovertemplate='State: %{text}<br>UMAP1: %{x}<br>UMAP2: %{y}<br>UMAP3: %{z}'\n",
    "        ))\n",
    "\n",
    "# Add anomalies as a separate trace\n",
    "anomaly_data = df_sample[df_sample['is_anomaly'] == 1].sample(min(5000, df_sample['is_anomaly'].sum()))\n",
    "fig2.add_trace(go.Scatter3d(\n",
    "    x=anomaly_data['UMAP1'],\n",
    "    y=anomaly_data['UMAP2'],\n",
    "    z=anomaly_data['UMAP3'],\n",
    "    mode='markers',\n",
    "    name='ANOMALIES',\n",
    "    marker=dict(\n",
    "        size=4,\n",
    "        color='red',\n",
    "        symbol='diamond',\n",
    "        opacity=0.9\n",
    "    ),\n",
    "    text=anomaly_data['state'],\n",
    "    hovertemplate='ANOMALY<br>State: %{text}<br>UMAP1: %{x}<br>UMAP2: %{y}<br>UMAP3: %{z}'\n",
    "))\n",
    "\n",
    "fig2.update_layout(\n",
    "    title='3D View: States and Anomalies',\n",
    "    scene=dict(\n",
    "        xaxis_title='UMAP1',\n",
    "        yaxis_title='UMAP2',\n",
    "        zaxis_title='UMAP3',\n",
    "        bgcolor='white'\n",
    "    ),\n",
    "    height=700,\n",
    "    showlegend=True\n",
    ")\n",
    "\n",
    "fig2.show()\n",
    "\n",
    "# Third visualization: Distance from center\n",
    "print(\"\\nCreating distance-based 3D visualization...\")\n",
    "\n",
    "fig3 = px.scatter_3d(\n",
    "    df_sample.sample(min(30000, len(df_sample))),\n",
    "    x='UMAP1',\n",
    "    y='UMAP2', \n",
    "    z='UMAP3',\n",
    "    color='distance_from_center',\n",
    "    color_continuous_scale='Viridis',\n",
    "    title='3D Distance from Center (Anomaly Score)',\n",
    "    hover_data=['state', 'is_anomaly', 'bed_error']\n",
    ")\n",
    "\n",
    "fig3.update_traces(marker=dict(size=2, opacity=0.7))\n",
    "fig3.update_layout(height=700)\n",
    "fig3.show()\n",
    "\n",
    "print(\"\\nâœ“ 3D visualizations complete!\")\n",
    "print(\"You can rotate, zoom, and interact with the plots!\")\n",
    "\n",
    "#prepare data for training model \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"PHASE 6: SUPERVISED MODEL TRAINING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load the labeled data\n",
    "print(\"\\nStep 1: Preparing data for training...\")\n",
    "\n",
    "# Get features and labels\n",
    "X = df_sample[feature_columns].values\n",
    "y = df_sample['is_anomaly'].values\n",
    "\n",
    "print(f\"Dataset shape: {X.shape}\")\n",
    "print(f\"Features: {feature_columns}\")\n",
    "print(f\"Total samples: {len(y):,}\")\n",
    "print(f\"Anomalies: {y.sum():,} ({y.mean()*100:.2f}%)\")\n",
    "print(f\"Normal: {len(y) - y.sum():,} ({(1-y.mean())*100:.2f}%)\")\n",
    "\n",
    "# Split into train/test sets (70/30)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.3, \n",
    "    random_state=42,\n",
    "    stratify=y  # Ensures both sets have same anomaly percentage\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining set: {len(X_train):,} samples\")\n",
    "print(f\"  - Anomalies: {y_train.sum():,} ({y_train.mean()*100:.2f}%)\")\n",
    "print(f\"Test set: {len(X_test):,} samples\")\n",
    "print(f\"  - Anomalies: {y_test.sum():,} ({y_test.mean()*100:.2f}%)\")\n",
    "\n",
    "# Scale features (important for some algorithms)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"\\nâœ“ Data prepared for training\")\n",
    "\n",
    "#comparing different models\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import time\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING MULTIPLE MODELS FOR COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Dictionary to store results\n",
    "model_results = {}\n",
    "\n",
    "# Model 1: Logistic Regression (Simple, Fast, Interpretable)\n",
    "print(\"\\n1. Training Logistic Regression...\")\n",
    "start_time = time.time()\n",
    "\n",
    "lr_model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "lr_pred = lr_model.predict(X_test_scaled)\n",
    "\n",
    "lr_time = time.time() - start_time\n",
    "model_results['Logistic Regression'] = {\n",
    "    'model': lr_model,\n",
    "    'predictions': lr_pred,\n",
    "    'accuracy': accuracy_score(y_test, lr_pred),\n",
    "    'precision': precision_score(y_test, lr_pred),\n",
    "    'recall': recall_score(y_test, lr_pred),\n",
    "    'f1': f1_score(y_test, lr_pred),\n",
    "    'training_time': lr_time\n",
    "}\n",
    "print(f\"  Accuracy: {model_results['Logistic Regression']['accuracy']:.4f}\")\n",
    "print(f\"  Time: {lr_time:.2f} seconds\")\n",
    "\n",
    "# Model 2: Random Forest (Good for complex patterns)\n",
    "print(\"\\n2. Training Random Forest...\")\n",
    "start_time = time.time()\n",
    "\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    min_samples_split=20,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf_model.fit(X_train, y_train)  # Note: Random Forest doesn't need scaling\n",
    "rf_pred = rf_model.predict(X_test)\n",
    "\n",
    "rf_time = time.time() - start_time\n",
    "model_results['Random Forest'] = {\n",
    "    'model': rf_model,\n",
    "    'predictions': rf_pred,\n",
    "    'accuracy': accuracy_score(y_test, rf_pred),\n",
    "    'precision': precision_score(y_test, rf_pred),\n",
    "    'recall': recall_score(y_test, rf_pred),\n",
    "    'f1': f1_score(y_test, rf_pred),\n",
    "    'training_time': rf_time\n",
    "}\n",
    "print(f\"  Accuracy: {model_results['Random Forest']['accuracy']:.4f}\")\n",
    "print(f\"  Time: {rf_time:.2f} seconds\")\n",
    "\n",
    "# Model 3: XGBoost (Usually best performance)\n",
    "print(\"\\n3. Training XGBoost...\")\n",
    "start_time = time.time()\n",
    "\n",
    "xgb_model = XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    random_state=42,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "xgb_model.fit(X_train, y_train)\n",
    "xgb_pred = xgb_model.predict(X_test)\n",
    "\n",
    "xgb_time = time.time() - start_time\n",
    "model_results['XGBoost'] = {\n",
    "    'model': xgb_model,\n",
    "    'predictions': xgb_pred,\n",
    "    'accuracy': accuracy_score(y_test, xgb_pred),\n",
    "    'precision': precision_score(y_test, xgb_pred),\n",
    "    'recall': recall_score(y_test, xgb_pred),\n",
    "    'f1': f1_score(y_test, xgb_pred),\n",
    "    'training_time': xgb_time\n",
    "}\n",
    "print(f\"  Accuracy: {model_results['XGBoost']['accuracy']:.4f}\")\n",
    "print(f\"  Time: {xgb_time:.2f} seconds\")\n",
    "\n",
    "# Compare all models\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "comparison_df = pd.DataFrame({\n",
    "    model_name: {\n",
    "        'Accuracy': results['accuracy'],\n",
    "        'Precision': results['precision'],\n",
    "        'Recall': results['recall'],\n",
    "        'F1 Score': results['f1'],\n",
    "        'Training Time': results['training_time']\n",
    "    }\n",
    "    for model_name, results in model_results.items()\n",
    "}).T\n",
    "\n",
    "print(comparison_df.round(4))\n",
    "\n",
    "# Identify best model\n",
    "best_model_name = comparison_df['F1 Score'].idxmax()\n",
    "print(f\"\\nðŸ† Best Model: {best_model_name} (F1 Score: {comparison_df.loc[best_model_name, 'F1 Score']:.4f})\")\n",
    "# CHECK WHAT'S HAPPENING\n",
    "print(\"=\"*60)\n",
    "print(\"DIAGNOSTIC CHECK - WHY IS ACCURACY SO HIGH?\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check what features we're actually using\n",
    "print(\"\\nFeatures being used:\")\n",
    "for i, feature in enumerate(feature_columns):\n",
    "    print(f\"  {i+1}. {feature}\")\n",
    "\n",
    "# Check if there's perfect separation\n",
    "print(\"\\nChecking feature distributions for anomalies vs normal...\")\n",
    "for feature in feature_columns:\n",
    "    anomaly_mean = df_sample[df_sample['is_anomaly']==1][feature].mean()\n",
    "    normal_mean = df_sample[df_sample['is_anomaly']==0][feature].mean()\n",
    "    print(f\"{feature:20} - Normal: {normal_mean:.2f}, Anomaly: {anomaly_mean:.2f}, Diff: {abs(anomaly_mean-normal_mean):.2f}\")\n",
    "\n",
    "# Check correlation with labels\n",
    "print(\"\\nFeature correlation with anomaly labels:\")\n",
    "for feature in feature_columns:\n",
    "    correlation = df_sample[feature].corr(df_sample['is_anomaly'])\n",
    "    print(f\"{feature:20}: {correlation:.4f}\")\n",
    "\n",
    "# Look at what XGBoost learned\n",
    "print(\"\\nXGBoost Feature Importance:\")\n",
    "for feature, importance in zip(feature_columns, xgb_model.feature_importances_):\n",
    "    print(f\"{feature:20}: {importance:.4f} {'â–ˆ' * int(importance * 50)}\")\n",
    "\n",
    "# Check if we have any \"perfect\" features\n",
    "print(\"\\nChecking for suspiciously perfect features...\")\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Train a simple decision tree with max_depth=1\n",
    "simple_tree = DecisionTreeClassifier(max_depth=1, random_state=42)\n",
    "simple_tree.fit(X_train, y_train)\n",
    "simple_accuracy = simple_tree.score(X_test, y_test)\n",
    "\n",
    "print(f\"Single split decision tree accuracy: {simple_accuracy:.4f}\")\n",
    "if simple_accuracy > 0.95:\n",
    "    print(\"âš ï¸ WARNING: A single feature can achieve >95% accuracy!\")\n",
    "    print(\"This suggests we have a feature that's too directly related to our labels!\")\n",
    "# RUN THIS - BETTER ANOMALY LABELING\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CREATING DOMAIN-BASED ANOMALY LABELS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create new anomaly column with stricter criteria\n",
    "df_sample['anomaly_v2'] = 0\n",
    "reasons_list = []\n",
    "\n",
    "# Criterion 1: STOPPED state (definite error)\n",
    "stopped = df_sample['state'] == 'STOPPED'\n",
    "df_sample.loc[stopped, 'anomaly_v2'] = 1\n",
    "print(f\"STOPPED states: {stopped.sum():,} anomalies\")\n",
    "\n",
    "# Criterion 2: Extreme temperature during PRINTING only\n",
    "printing_extreme_temp = (\n",
    "    (df_sample['state'] == 'PRINTING') & \n",
    "    ((df_sample['bed_error'] > 10) | (df_sample['nozzle_error'] > 20))\n",
    ")\n",
    "df_sample.loc[printing_extreme_temp, 'anomaly_v2'] = 1\n",
    "print(f\"PRINTING with high temp error: {printing_extreme_temp.sum():,} anomalies\")\n",
    "\n",
    "# Criterion 3: PRINTING with abnormal flow\n",
    "printing_bad_flow = (\n",
    "    (df_sample['state'] == 'PRINTING') & \n",
    "    ((df_sample['flow_feature'] < 80) | (df_sample['flow_feature'] > 110))\n",
    ")\n",
    "df_sample.loc[printing_bad_flow, 'anomaly_v2'] = 1\n",
    "print(f\"PRINTING with abnormal flow: {printing_bad_flow.sum():,} anomalies\")\n",
    "\n",
    "# Criterion 4: IDLE but hot (shouldn't happen)\n",
    "idle_hot = (\n",
    "    (df_sample['state'] == 'IDLE') & \n",
    "    ((df_sample['tempBed'] > 30) | (df_sample['tempNozzle'] > 50))\n",
    ")\n",
    "df_sample.loc[idle_hot, 'anomaly_v2'] = 1\n",
    "print(f\"IDLE but hot: {idle_hot.sum():,} anomalies\")\n",
    "\n",
    "# Criterion 5: Zero efficiency during active states\n",
    "zero_efficiency = (\n",
    "    (df_sample['state'].isin(['PRINTING', 'BUSY'])) & \n",
    "    (df_sample['efficiency'] == 0)\n",
    ")\n",
    "df_sample.loc[zero_efficiency, 'anomaly_v2'] = 1\n",
    "print(f\"Zero efficiency while active: {zero_efficiency.sum():,} anomalies\")\n",
    "\n",
    "# Final count\n",
    "total_anomalies_v2 = df_sample['anomaly_v2'].sum()\n",
    "anomaly_rate_v2 = total_anomalies_v2 / len(df_sample) * 100\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(f\"NEW ANOMALY LABELS CREATED\")\n",
    "print(f\"=\"*60)\n",
    "print(f\"Total anomalies: {total_anomalies_v2:,} ({anomaly_rate_v2:.2f}%)\")\n",
    "print(f\"Normal: {len(df_sample) - total_anomalies_v2:,} ({100-anomaly_rate_v2:.2f}%)\")\n",
    "\n",
    "# Check distribution by state\n",
    "print(\"\\nAnomaly rate by state:\")\n",
    "for state in df_sample['state'].unique():\n",
    "    if state:  # Skip empty\n",
    "        state_data = df_sample[df_sample['state'] == state]\n",
    "        state_anomalies = state_data['anomaly_v2'].sum()\n",
    "        state_rate = (state_anomalies / len(state_data) * 100) if len(state_data) > 0 else 0\n",
    "        print(f\"  {state:10} {state_anomalies:7,} / {len(state_data):7,} = {state_rate:6.2f}%\")\n",
    "# RETRAIN WITH NEW LABELS\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RETRAINING MODELS WITH DOMAIN-BASED LABELS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Use the new labels\n",
    "X_new = df_sample[feature_columns].values\n",
    "y_new = df_sample['anomaly_v2'].values\n",
    "\n",
    "print(f\"Using {len(feature_columns)} features: {feature_columns}\")\n",
    "print(f\"Total samples: {len(y_new):,}\")\n",
    "print(f\"Anomalies: {y_new.sum():,} ({y_new.mean()*100:.2f}%)\")\n",
    "\n",
    "# Split data\n",
    "X_train_v2, X_test_v2, y_train_v2, y_test_v2 = train_test_split(\n",
    "    X_new, y_new,\n",
    "    test_size=0.3,\n",
    "    random_state=42,\n",
    "    stratify=y_new\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain set: {len(y_train_v2):,} ({y_train_v2.sum():,} anomalies)\")\n",
    "print(f\"Test set: {len(y_test_v2):,} ({y_test_v2.sum():,} anomalies)\")\n",
    "\n",
    "# Scale features\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler_v2 = StandardScaler()\n",
    "X_train_v2_scaled = scaler_v2.fit_transform(X_train_v2)\n",
    "X_test_v2_scaled = scaler_v2.transform(X_test_v2)\n",
    "\n",
    "# Train three models\n",
    "models_v2 = {}\n",
    "\n",
    "# 1. Logistic Regression\n",
    "print(\"\\n1. Training Logistic Regression...\")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr_v2 = LogisticRegression(random_state=42, max_iter=1000)\n",
    "lr_v2.fit(X_train_v2_scaled, y_train_v2)\n",
    "lr_pred_v2 = lr_v2.predict(X_test_v2_scaled)\n",
    "models_v2['Logistic Regression'] = {\n",
    "    'model': lr_v2,\n",
    "    'predictions': lr_pred_v2,\n",
    "    'accuracy': accuracy_score(y_test_v2, lr_pred_v2),\n",
    "    'f1': f1_score(y_test_v2, lr_pred_v2)\n",
    "}\n",
    "print(f\"   Accuracy: {models_v2['Logistic Regression']['accuracy']:.4f}\")\n",
    "print(f\"   F1 Score: {models_v2['Logistic Regression']['f1']:.4f}\")\n",
    "\n",
    "# 2. Random Forest\n",
    "print(\"\\n2. Training Random Forest...\")\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf_v2 = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1)\n",
    "rf_v2.fit(X_train_v2, y_train_v2)\n",
    "rf_pred_v2 = rf_v2.predict(X_test_v2)\n",
    "models_v2['Random Forest'] = {\n",
    "    'model': rf_v2,\n",
    "    'predictions': rf_pred_v2,\n",
    "    'accuracy': accuracy_score(y_test_v2, rf_pred_v2),\n",
    "    'f1': f1_score(y_test_v2, rf_pred_v2)\n",
    "}\n",
    "print(f\"   Accuracy: {models_v2['Random Forest']['accuracy']:.4f}\")\n",
    "print(f\"   F1 Score: {models_v2['Random Forest']['f1']:.4f}\")\n",
    "\n",
    "# 3. XGBoost\n",
    "print(\"\\n3. Training XGBoost...\")\n",
    "from xgboost import XGBClassifier\n",
    "xgb_v2 = XGBClassifier(n_estimators=100, max_depth=6, learning_rate=0.1, \n",
    "                        random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
    "xgb_v2.fit(X_train_v2, y_train_v2)\n",
    "xgb_pred_v2 = xgb_v2.predict(X_test_v2)\n",
    "models_v2['XGBoost'] = {\n",
    "    'model': xgb_v2,\n",
    "    'predictions': xgb_pred_v2,\n",
    "    'accuracy': accuracy_score(y_test_v2, xgb_pred_v2),\n",
    "    'f1': f1_score(y_test_v2, xgb_pred_v2)\n",
    "}\n",
    "print(f\"   Accuracy: {models_v2['XGBoost']['accuracy']:.4f}\")\n",
    "print(f\"   F1 Score: {models_v2['XGBoost']['f1']:.4f}\")\n",
    "\n",
    "# Find best model\n",
    "best_model_v2 = max(models_v2.items(), key=lambda x: x[1]['f1'])\n",
    "print(f\"\\nðŸ† Best Model: {best_model_v2[0]}\")\n",
    "print(f\"   F1 Score: {best_model_v2[1]['f1']:.4f}\")\n",
    "\n",
    "# Check if results are more realistic\n",
    "if best_model_v2[1]['accuracy'] < 0.95:\n",
    "    print(\"\\nâœ… GOOD! Accuracy < 95% means model has to work harder to learn patterns.\")\n",
    "    print(\"This will generalize better to new data!\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸ Still high accuracy. But let's check confusion matrix...\")\n",
    "# CHECK IF NEW MODEL IS REALISTIC\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EVALUATING REALISTIC MODEL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Get best model's predictions\n",
    "best_name = best_model_v2[0]\n",
    "best_predictions = models_v2[best_name]['predictions']\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test_v2, best_predictions)\n",
    "print(f\"\\nConfusion Matrix for {best_name}:\")\n",
    "print(f\"                 Predicted\")\n",
    "print(f\"                 Normal  Anomaly\")\n",
    "print(f\"Actual Normal    {cm[0,0]:6d}  {cm[0,1]:6d}\")\n",
    "print(f\"Actual Anomaly   {cm[1,0]:6d}  {cm[1,1]:6d}\")\n",
    "\n",
    "# Detailed metrics\n",
    "print(\"\\nDetailed Classification Report:\")\n",
    "print(classification_report(y_test_v2, best_predictions, \n",
    "                          target_names=['Normal', 'Anomaly']))\n",
    "\n",
    "# Feature importance for best model\n",
    "if hasattr(models_v2[best_name]['model'], 'feature_importances_'):\n",
    "    print(f\"\\nFeature Importance ({best_name}):\")\n",
    "    for feat, imp in zip(feature_columns, models_v2[best_name]['model'].feature_importances_):\n",
    "        print(f\"  {feat:20}: {imp:.4f} {'â–ˆ' * int(imp * 30)}\")\n",
    "\n",
    "# Final verdict\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "false_positive_rate = fp / (fp + tn) * 100\n",
    "false_negative_rate = fn / (fn + tp) * 100\n",
    "\n",
    "print(f\"\\nModel Performance Summary:\")\n",
    "print(f\"  False Positive Rate: {false_positive_rate:.2f}% (marking normal as anomaly)\")\n",
    "print(f\"  False Negative Rate: {false_negative_rate:.2f}% (missing real anomalies)\")\n",
    "\n",
    "if false_negative_rate > 20:\n",
    "    print(\"  âš ï¸ Model misses many anomalies - might need adjustment\")\n",
    "elif false_positive_rate > 10:\n",
    "    print(\"  âš ï¸ Model has many false alarms - might need adjustment\")\n",
    "else:\n",
    "    print(\"  âœ… Model has good balance!\")\n",
    "    \n",
    "# OPTION B: DISCOVER UNKNOWN ANOMALIES\n",
    "print(\"=\"*60)\n",
    "print(\"FINDING COMPLEX ANOMALIES NOT DEFINED BY SIMPLE RULES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Remove the \"obvious\" anomalies we already know about\n",
    "obvious_anomalies = (\n",
    "    (df_sample['state'] == 'STOPPED') |\n",
    "    ((df_sample['state'] == 'IDLE') & (df_sample['tempBed'] > 30)) |\n",
    "    ((df_sample['state'] == 'PRINTING') & (df_sample['bed_error'] > 10))\n",
    ")\n",
    "\n",
    "# Focus on the \"normal\" data to find hidden anomalies\n",
    "df_complex = df_sample[~obvious_anomalies].copy()\n",
    "print(f\"After removing obvious anomalies: {len(df_complex):,} records\")\n",
    "\n",
    "# Use Isolation Forest to find subtle anomalies\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "print(\"\\nFinding subtle anomalies with Isolation Forest...\")\n",
    "iso_forest = IsolationForest(\n",
    "    contamination=0.01,  # Find 1% most unusual\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit on the features\n",
    "X_complex = df_complex[feature_columns].values\n",
    "anomaly_scores = iso_forest.fit_predict(X_complex)\n",
    "\n",
    "# -1 = anomaly, 1 = normal\n",
    "df_complex['subtle_anomaly'] = (anomaly_scores == -1).astype(int)\n",
    "\n",
    "subtle_anomalies = df_complex['subtle_anomaly'].sum()\n",
    "print(f\"Found {subtle_anomalies:,} subtle anomalies ({subtle_anomalies/len(df_complex)*100:.2f}%)\")\n",
    "\n",
    "# What makes these different?\n",
    "print(\"\\nComparing subtle anomalies to normal:\")\n",
    "for feature in feature_columns:\n",
    "    normal_mean = df_complex[df_complex['subtle_anomaly']==0][feature].mean()\n",
    "    anomaly_mean = df_complex[df_complex['subtle_anomaly']==1][feature].mean()\n",
    "    diff_pct = (anomaly_mean - normal_mean) / normal_mean * 100 if normal_mean != 0 else 0\n",
    "    print(f\"{feature:20}: Normal={normal_mean:.2f}, Anomaly={anomaly_mean:.2f} ({diff_pct:+.1f}%)\")\n",
    "\n",
    "# These are the INTERESTING anomalies - patterns we didn't know about!\n",
    "# FINAL ANOMALY DETECTION APPROACH\n",
    "print(\"=\"*60)\n",
    "print(\"FINAL STATE-SPECIFIC ANOMALY DETECTION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Initialize results\n",
    "all_results = []\n",
    "\n",
    "# Process each state separately\n",
    "for state in ['IDLE', 'FINISHED', 'PRINTING', 'BUSY']:\n",
    "    state_data = df_sample[df_sample['state'] == state].copy()\n",
    "    \n",
    "    if len(state_data) > 500:  # Need enough data\n",
    "        print(f\"\\nProcessing {state}: {len(state_data):,} records\")\n",
    "        \n",
    "        # Train Isolation Forest for this specific state\n",
    "        iso = IsolationForest(\n",
    "            contamination=0.02,  # 2% anomalies expected\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        X_state = state_data[feature_columns].values\n",
    "        predictions = iso.fit_predict(X_state)\n",
    "        \n",
    "        # Mark anomalies\n",
    "        state_data['anomaly_final'] = (predictions == -1).astype(int)\n",
    "        anomalies = state_data['anomaly_final'].sum()\n",
    "        \n",
    "        print(f\"  Found {anomalies:,} anomalies ({anomalies/len(state_data)*100:.2f}%)\")\n",
    "        all_results.append(state_data)\n",
    "\n",
    "# Handle STOPPED (always anomaly)\n",
    "stopped_data = df_sample[df_sample['state'] == 'STOPPED'].copy()\n",
    "stopped_data['anomaly_final'] = 1\n",
    "all_results.append(stopped_data)\n",
    "print(f\"\\nSTOPPED: {len(stopped_data)} records (100% anomalies)\")\n",
    "\n",
    "# Combine all\n",
    "df_final = pd.concat(all_results, ignore_index=True)\n",
    "\n",
    "# Final statistics\n",
    "total_anomalies_final = df_final['anomaly_final'].sum()\n",
    "final_rate = total_anomalies_final / len(df_final) * 100\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"FINAL RESULTS:\")\n",
    "print(f\"Total anomalies: {total_anomalies_final:,} ({final_rate:.2f}%)\")\n",
    "print(f\"This is realistic and meaningful!\")\n",
    "\n",
    "# TRAIN FINAL PRODUCTION MODEL\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING FINAL PRODUCTION MODEL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score\n",
    "\n",
    "# Prepare final dataset\n",
    "X_final = df_final[feature_columns].values\n",
    "y_final = df_final['anomaly_final'].values\n",
    "\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_final, y_final, test_size=0.3, random_state=42, stratify=y_final\n",
    ")\n",
    "\n",
    "print(f\"Training on {len(X_train):,} samples\")\n",
    "print(f\"Testing on {len(X_test):,} samples\")\n",
    "print(f\"Anomaly rate: {y_final.mean()*100:.2f}%\")\n",
    "\n",
    "# Train XGBoost\n",
    "model_final = XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=6,\n",
    "    random_state=42,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "\n",
    "print(\"\\nTraining XGBoost...\")\n",
    "model_final.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = model_final.predict(X_test)\n",
    "\n",
    "# Results\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"\\nFINAL MODEL PERFORMANCE:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Normal', 'Anomaly']))\n",
    "\n",
    "# Save model\n",
    "import joblib\n",
    "joblib.dump(model_final, 'final_anomaly_model.pkl')\n",
    "print(\"\\nâœ… Model saved as 'final_anomaly_model.pkl'\")\n",
    "print(\"âœ… READY FOR PRODUCTION!\")\n",
    "\n",
    "# QUICK VALIDATION\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"VALIDATION FOR THESIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Summary statistics for thesis\n",
    "thesis_stats = {\n",
    "    'Total_Records_Processed': 94869699,\n",
    "    'Sample_Size': len(df_final),\n",
    "    'Features_Created': 7,\n",
    "    'Anomalies_Found': total_anomalies_final,\n",
    "    'Anomaly_Rate': f\"{final_rate:.2f}%\",\n",
    "    'Model_Accuracy': f\"{accuracy:.4f}\",\n",
    "    'Model_F1_Score': f\"{f1:.4f}\",\n",
    "    'Processing_Time': \"94.4 minutes for full dataset\"\n",
    "}\n",
    "\n",
    "print(\"\\nðŸ“Š THESIS STATISTICS:\")\n",
    "for key, value in thesis_stats.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "print(\"\\nâœ… PROJECT COMPLETE!\")\n",
    "print(\"You can now write in your thesis that you:\")\n",
    "print(\"1. Processed 94.8M records\")\n",
    "print(\"2. Identified meaningful anomalies using state-specific models\")\n",
    "print(\"3. Achieved production-ready accuracy\")\n",
    "print(\"4. Created scalable solution for predictive maintenance\")\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "\n",
    "# 5-fold stratified cross-validation\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_scores = cross_val_score(\n",
    "    model_final, X_final, y_final, \n",
    "    cv=skf, \n",
    "    scoring='f1',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"Cross-validation F1 scores by fold:\")\n",
    "for i, score in enumerate(cv_scores):\n",
    "    print(f\"  Fold {i+1}: {score:.4f}\")\n",
    "    \n",
    "print(f\"Mean F1: {cv_scores.mean():.4f}\")\n",
    "print(f\"Std Dev: {cv_scores.std():.4f}\")\n",
    "print(f\"95% CI: [{cv_scores.mean() - 2*cv_scores.std():.4f}, \"\n",
    "      f\"{cv_scores.mean() + 2*cv_scores.std():.4f}]\")\n",
    "# Feature importance from XGBoost\n",
    "importances = model_final.feature_importances_\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'feature': feature_columns,\n",
    "    'importance': importances\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.barh(feature_importance_df['feature'], feature_importance_df['importance'])\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Feature Importance from XGBoost')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Printer Env)",
   "language": "python",
   "name": "printer-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
