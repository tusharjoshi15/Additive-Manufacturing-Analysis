{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "225f3ed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 job list files to process.\n",
      "  - 2024-prusa-job-log.txt\n",
      "  - 2025-prusa-job-log.txt\n",
      "\n",
      "Finished processing job list files. Combined DataFrame has 1,408 rows and 5 columns.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import polars as pl\n",
    "import glob # Import glob to find multiple files\n",
    "\n",
    "# --- Configuration ---\n",
    "# IMPORTANT: Update this path to the folder containing your job list .txt files.\n",
    "# This folder should contain '2024-prusa-job-log.txt' and '2025-prusa-job-log.txt'.\n",
    "job_list_folder_path = \"/Users/tusharjoshi/Desktop/ProjectWorkAll/Dissertation /DATA/\" \n",
    "\n",
    "# Define the final order of columns for the job list DataFrame\n",
    "job_list_final_order = [\"date\", \"id\", \"name\", \"size\", \"mTimestamp\"]\n",
    "\n",
    "# --- Main Processing Logic ---\n",
    "# Find all .txt files in the specified folder\n",
    "all_job_list_files = sorted(glob.glob(os.path.join(job_list_folder_path, \"*.txt\")))\n",
    "\n",
    "if not all_job_list_files:\n",
    "    print(f\"No .txt files found in {job_list_folder_path}. Please check the path and file extensions. Exiting.\")\n",
    "else:\n",
    "    print(f\"Found {len(all_job_list_files)} job list files to process.\")\n",
    "    for file_path in all_job_list_files:\n",
    "        print(f\"  - {os.path.basename(file_path)}\")\n",
    "\n",
    "    # Read all NDJSON files into a single Polars DataFrame\n",
    "    # Polars will infer initial types, then we'll explicitly cast 'date'.\n",
    "    df_job_list_processed = pl.read_ndjson(all_job_list_files)\n",
    "\n",
    "    # Parse the 'date' column from string to actual Datetime objects.\n",
    "    # 'strict=False' means if a date can't be parsed, it becomes NULL instead of crashing.\n",
    "    df_job_list_processed = df_job_list_processed.with_columns(\n",
    "        pl.col(\"date\").str.strptime(pl.Datetime, format=\"%Y-%m-%dT%H:%M:%S%.f%Z\", strict=False).alias(\"date\")\n",
    "    )\n",
    "\n",
    "    # Select and ensure final column order.\n",
    "    # This also handles cases where columns might be missing by filling with nulls if needed,\n",
    "    # though for job list files, we expect all columns to be present.\n",
    "    df_job_list_processed = df_job_list_processed.select(job_list_final_order)\n",
    "\n",
    "    print(f\"\\nFinished processing job list files. Combined DataFrame has {df_job_list_processed.shape[0]:,} rows and {len(df_job_list_processed.columns)} columns.\")\n",
    "    # The 'df_job_list_processed' DataFrame now holds your cleaned job list data.\n",
    "    # We will use this DataFrame in the next step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57c2c7c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Exporting job list data to MySQL database 'printer_data_db' table 'JobList'...\n",
      "Created table 'JobList' in MySQL.\n",
      "  Inserted 1000 rows. Total inserted: 1000\n",
      "  Inserted 408 rows. Total inserted: 1408\n",
      "Successfully exported all job list data to MySQL table 'JobList'.\n",
      "MySQL connection closed.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pymysql\n",
    "from sqlalchemy import create_engine, text # Used for robust connection\n",
    "\n",
    "# This code block assumes 'df_job_list_processed' is available from the previous Step 1.\n",
    "# If you are running this in a new session, you must run Step 1's code first.\n",
    "\n",
    "# --- Configuration ---\n",
    "# MySQL Database Connection Details (Same as before)\n",
    "DB_HOST = \"localhost\"\n",
    "DB_USER = \"root\"       # Your MySQL root username\n",
    "DB_PASSWORD = \"admintushar15\" # Replace with your MySQL root password\n",
    "DB_NAME = \"printer_data_db\" # The database name you created in DBeaver\n",
    "JOB_LIST_TABLE_NAME = \"JobList\" # New table name for the job list data\n",
    "\n",
    "# --- Main Export Process to MySQL ---\n",
    "if 'df_job_list_processed' not in locals():\n",
    "    print(\"Error: 'df_job_list_processed' DataFrame not found. Please run Step 1 first.\")\n",
    "else:\n",
    "    try:\n",
    "        # Connect to MySQL using pymysql\n",
    "        conn = pymysql.connect(host=DB_HOST, user=DB_USER, password=DB_PASSWORD, database=DB_NAME)\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        print(f\"\\nExporting job list data to MySQL database '{DB_NAME}' table '{JOB_LIST_TABLE_NAME}'...\")\n",
    "\n",
    "        # Drop table if it exists to ensure a clean export\n",
    "        cursor.execute(f\"DROP TABLE IF EXISTS `{JOB_LIST_TABLE_NAME}`;\")\n",
    "        \n",
    "        # Create table schema in MySQL based on Polars DataFrame columns and types\n",
    "        # Map Polars types to MySQL types\n",
    "        type_mapping = {\n",
    "            pl.String: \"TEXT\",\n",
    "            pl.Int64: \"BIGINT\", # Use BIGINT for large integers like size and mTimestamp\n",
    "            pl.Datetime: \"DATETIME(6)\" # DATETIME(6) for microsecond precision\n",
    "        }\n",
    "        columns_sql_defs = []\n",
    "        for col_name, dtype in df_job_list_processed.schema.items():\n",
    "            mysql_type = type_mapping.get(dtype, \"TEXT\") \n",
    "            columns_sql_defs.append(f'`{col_name}` {mysql_type}') \n",
    "        \n",
    "        create_table_sql = f\"CREATE TABLE `{JOB_LIST_TABLE_NAME}` ({', '.join(columns_sql_defs)});\"\n",
    "        cursor.execute(create_table_sql)\n",
    "        print(f\"Created table '{JOB_LIST_TABLE_NAME}' in MySQL.\")\n",
    "\n",
    "        # Prepare the INSERT statement\n",
    "        placeholders = \", \".join([\"%s\" for _ in df_job_list_processed.columns]) # Use %s for pymysql placeholders\n",
    "        insert_sql = f\"INSERT INTO `{JOB_LIST_TABLE_NAME}` VALUES ({placeholders});\"\n",
    "\n",
    "        # Iterate over the Polars DataFrame in batches and insert\n",
    "        batch_size = 1000 # Smaller batch size for job list as it's smaller overall\n",
    "        total_inserted_rows = 0\n",
    "        for batch_start in range(0, df_job_list_processed.shape[0], batch_size):\n",
    "            batch_end = min(batch_start + batch_size, df_job_list_processed.shape[0])\n",
    "            df_batch_to_insert = df_job_list_processed.slice(batch_start, batch_end - batch_start)\n",
    "            \n",
    "            # Convert Polars batch to a list of tuples for pymysql.executemany\n",
    "            rows_to_insert = []\n",
    "            for row_data in df_batch_to_insert.iter_rows():\n",
    "                formatted_row = []\n",
    "                for col_val, col_name in zip(row_data, df_job_list_processed.columns): \n",
    "                    if isinstance(col_val, pl.Datetime):\n",
    "                        formatted_row.append(col_val.to_pydatetime()) # Convert Polars Datetime to Python datetime\n",
    "                    else:\n",
    "                        formatted_row.append(col_val)\n",
    "                rows_to_insert.append(tuple(formatted_row))\n",
    "\n",
    "            cursor.executemany(insert_sql, rows_to_insert)\n",
    "            conn.commit() # Commit after each batch\n",
    "            total_inserted_rows += len(rows_to_insert)\n",
    "            print(f\"  Inserted {len(rows_to_insert)} rows. Total inserted: {total_inserted_rows}\")\n",
    "\n",
    "        print(f\"Successfully exported all job list data to MySQL table '{JOB_LIST_TABLE_NAME}'.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn error occurred during the MySQL export process: {e}\")\n",
    "        print(\"Please check:\")\n",
    "        print(\"- MySQL server is running and accessible.\")\n",
    "        print(\"- Database name, username, and password are correct.\")\n",
    "        print(\"- You have sufficient privileges to create/write to the table.\")\n",
    "\n",
    "    finally:\n",
    "        if 'conn' in locals() and conn.open: \n",
    "            conn.close()\n",
    "            print(\"MySQL connection closed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85b82afa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating joined table 'PrinterData_with_Jobs' in 'printer_data_db'...\n"
     ]
    },
    {
     "ename": "OperationalError",
     "evalue": "(2013, 'Lost connection to MySQL server during query')",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOperationalError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 49\u001b[39m\n\u001b[32m     45\u001b[39m cursor.execute(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDROP TABLE IF EXISTS `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mJOINED_TABLE_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m`;\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     47\u001b[39m \u001b[38;5;66;03m# Execute the SQL query to create and populate the joined table\u001b[39;00m\n\u001b[32m     48\u001b[39m \u001b[38;5;66;03m# This is the command that will take a long time to run on the MySQL server.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m \u001b[43mcursor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcreate_joined_table_sql\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m conn.commit() \u001b[38;5;66;03m# Save the changes to the database\u001b[39;00m\n\u001b[32m     52\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSuccessfully created and populated joined table \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mJOINED_TABLE_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/pymysql/cursors.py:153\u001b[39m, in \u001b[36mCursor.execute\u001b[39m\u001b[34m(self, query, args)\u001b[39m\n\u001b[32m    149\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    151\u001b[39m query = \u001b[38;5;28mself\u001b[39m.mogrify(query, args)\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    154\u001b[39m \u001b[38;5;28mself\u001b[39m._executed = query\n\u001b[32m    155\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/pymysql/cursors.py:322\u001b[39m, in \u001b[36mCursor._query\u001b[39m\u001b[34m(self, q)\u001b[39m\n\u001b[32m    320\u001b[39m conn = \u001b[38;5;28mself\u001b[39m._get_db()\n\u001b[32m    321\u001b[39m \u001b[38;5;28mself\u001b[39m._clear_result()\n\u001b[32m--> \u001b[39m\u001b[32m322\u001b[39m \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    323\u001b[39m \u001b[38;5;28mself\u001b[39m._do_get_result()\n\u001b[32m    324\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.rowcount\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/pymysql/connections.py:563\u001b[39m, in \u001b[36mConnection.query\u001b[39m\u001b[34m(self, sql, unbuffered)\u001b[39m\n\u001b[32m    561\u001b[39m     sql = sql.encode(\u001b[38;5;28mself\u001b[39m.encoding, \u001b[33m\"\u001b[39m\u001b[33msurrogateescape\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    562\u001b[39m \u001b[38;5;28mself\u001b[39m._execute_command(COMMAND.COM_QUERY, sql)\n\u001b[32m--> \u001b[39m\u001b[32m563\u001b[39m \u001b[38;5;28mself\u001b[39m._affected_rows = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_read_query_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43munbuffered\u001b[49m\u001b[43m=\u001b[49m\u001b[43munbuffered\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    564\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._affected_rows\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/pymysql/connections.py:825\u001b[39m, in \u001b[36mConnection._read_query_result\u001b[39m\u001b[34m(self, unbuffered)\u001b[39m\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    824\u001b[39m     result = MySQLResult(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m825\u001b[39m     \u001b[43mresult\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[38;5;28mself\u001b[39m._result = result\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m result.server_status \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/pymysql/connections.py:1199\u001b[39m, in \u001b[36mMySQLResult.read\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1197\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mread\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m   1198\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1199\u001b[39m         first_packet = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_read_packet\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1201\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m first_packet.is_ok_packet():\n\u001b[32m   1202\u001b[39m             \u001b[38;5;28mself\u001b[39m._read_ok_packet(first_packet)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/pymysql/connections.py:744\u001b[39m, in \u001b[36mConnection._read_packet\u001b[39m\u001b[34m(self, packet_type)\u001b[39m\n\u001b[32m    742\u001b[39m buff = \u001b[38;5;28mbytearray\u001b[39m()\n\u001b[32m    743\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m744\u001b[39m     packet_header = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_read_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    745\u001b[39m     \u001b[38;5;66;03m# if DEBUG: dump_packet(packet_header)\u001b[39;00m\n\u001b[32m    747\u001b[39m     btrl, btrh, packet_number = struct.unpack(\u001b[33m\"\u001b[39m\u001b[33m<HBB\u001b[39m\u001b[33m\"\u001b[39m, packet_header)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/pymysql/connections.py:798\u001b[39m, in \u001b[36mConnection._read_bytes\u001b[39m\u001b[34m(self, num_bytes)\u001b[39m\n\u001b[32m    796\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) < num_bytes:\n\u001b[32m    797\u001b[39m     \u001b[38;5;28mself\u001b[39m._force_close()\n\u001b[32m--> \u001b[39m\u001b[32m798\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m err.OperationalError(\n\u001b[32m    799\u001b[39m         CR.CR_SERVER_LOST, \u001b[33m\"\u001b[39m\u001b[33mLost connection to MySQL server during query\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    800\u001b[39m     )\n\u001b[32m    801\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[31mOperationalError\u001b[39m: (2013, 'Lost connection to MySQL server during query')"
     ]
    }
   ],
   "source": [
    "import pymysql\n",
    "\n",
    "# --- Configuration ---\n",
    "# IMPORTANT: Update these with your MySQL server details\n",
    "DB_HOST = \"localhost\"\n",
    "DB_USER = \"root\"       # Your MySQL root username\n",
    "DB_PASSWORD = \"admintushar15\" # Replace with your MySQL root password\n",
    "DB_NAME = \"printer_data_db\" # The database name you created\n",
    "\n",
    "MAIN_PRINTER_DATA_TABLE = \"PrinterData\" # Your 94M row table in MySQL\n",
    "JOB_LIST_TABLE = \"JobList\"             # Your 1.4K row table in MySQL\n",
    "JOINED_TABLE_NAME = \"PrinterData_with_Jobs\" # New table name for the joined data\n",
    "\n",
    "# --- SQL Query to Create the Joined Table ---\n",
    "# This query links each sensor record to the most recent job that started on that printer\n",
    "# at or before the sensor reading's timestamp.\n",
    "# It benefits greatly from the 'idx_joblist_id_date' index on JobList.\n",
    "create_joined_table_sql = f\"\"\"\n",
    "CREATE TABLE `{JOINED_TABLE_NAME}` AS\n",
    "SELECT\n",
    "    P.*, -- Selects all columns from the PrinterData table\n",
    "    J.name AS JobName, -- Selects the job name and renames it to JobName\n",
    "    J.size AS JobSize, -- Selects the job size and renames it to JobSize\n",
    "    J.mTimestamp AS JobmTimestamp -- Selects the job mTimestamp and renames it to JobmTimestamp\n",
    "FROM\n",
    "    `{MAIN_PRINTER_DATA_TABLE}` AS P -- Start with the main sensor data table, aliased as P\n",
    "LEFT JOIN `{JOB_LIST_TABLE}` AS J -- Join with the job list table, aliased as J\n",
    "    ON P.id = J.id -- Condition 1: Join only if the printer IDs match\n",
    "    AND J.date = ( -- Condition 2: Join if the job date is the MAX date that meets criteria\n",
    "        SELECT MAX(J2.date) -- Find the latest job date (MAX)\n",
    "        FROM `{JOB_LIST_TABLE}` AS J2 -- From the JobList table, aliased as J2\n",
    "        WHERE J2.id = P.id -- Where the job is for the same printer as the sensor record\n",
    "          AND J2.date <= P.date -- AND the job started at or before the sensor record's date\n",
    "    );\n",
    "\"\"\"\n",
    "\n",
    "# --- Main Process ---\n",
    "# Connect to MySQL\n",
    "conn = pymysql.connect(host=DB_HOST, user=DB_USER, password=DB_PASSWORD, database=DB_NAME)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "print(f\"Creating joined table '{JOINED_TABLE_NAME}' in '{DB_NAME}'...\")\n",
    "\n",
    "# Drop the joined table if it exists (ensures a clean start)\n",
    "cursor.execute(f\"DROP TABLE IF EXISTS `{JOINED_TABLE_NAME}`;\")\n",
    "\n",
    "# Execute the SQL query to create and populate the joined table\n",
    "# This is the command that will take a long time to run on the MySQL server.\n",
    "cursor.execute(create_joined_table_sql)\n",
    "conn.commit() # Save the changes to the database\n",
    "\n",
    "print(f\"Successfully created and populated joined table '{JOINED_TABLE_NAME}'.\")\n",
    "\n",
    "# Optional: Verify row count of the new joined table\n",
    "cursor.execute(f\"SELECT COUNT(*) FROM `{JOINED_TABLE_NAME}`;\")\n",
    "joined_row_count = cursor.fetchone()[0]\n",
    "print(f\"Total rows in '{JOINED_TABLE_NAME}': {joined_row_count:,}\")\n",
    "\n",
    "# Close the MySQL connection\n",
    "cursor.close()\n",
    "conn.close()\n",
    "print(\"MySQL connection closed.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Printer Env)",
   "language": "python",
   "name": "printer-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
